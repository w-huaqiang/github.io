<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>华强的技术小站</title>
  
  <subtitle>知行合一</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://huaqiang.art/"/>
  <updated>2020-12-23T07:44:33.080Z</updated>
  <id>http://huaqiang.art/</id>
  
  <author>
    <name>王华强</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DevOps三步工作法</title>
    <link href="http://huaqiang.art/2020/12/23/devops_setp/"/>
    <id>http://huaqiang.art/2020/12/23/devops_setp/</id>
    <published>2020-12-23T07:27:20.000Z</published>
    <updated>2020-12-23T07:44:33.080Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-简述"><a href="#1-简述" class="headerlink" title="1. 简述"></a>1. 简述</h2><p>三步工作法的第一步就是实现<strong>流动</strong>，即实现从开发到运维的工作，快速地从左向右流动。</p><p>第二步就是<strong>反馈</strong>，从左到右的过程中，每一步都有快速的反馈，避免问题的复发，能及时发现和修复问题。</p><p>第三部是<strong>持续学习</strong>，目的是要创建有创新精神和信任度搞的企业文化，支持动态、严格且科学的实验，从失败中不断学习。</p><a id="more"></a><img src="/assets/img/threesetp1.jpg" alt="敏捷开发" width="400" ><h2 id="2-流动原则"><a href="#2-流动原则" class="headerlink" title="2 流动原则"></a>2 流动原则</h2><p>在价值的交付过程中，往往是从开发流向运维，模糊的需求流向有价值的产品，所以工作的第一步就是要建立从开发到运维之间快速、平滑且能向用户交付价值的工作流。</p><p>当我们熟悉并了解了整个价值流之后，我们就能清晰的知道整个价值流体系中，瓶颈在哪里，从而可以从全局优化整个流动，而不是陷入局部优化。在局部优化中，我们往往关注功能的完成度、Bug的发现率和修复率、运维维护的可用性等指标。</p><p>在这一部分工作内容中，要主要关注几个核心点即可视化、任务拆分、在制品限制、减少人为交接次数、识别和改进约束点，消除浪费。</p><h3 id="2-1-可视化"><a href="#2-1-可视化" class="headerlink" title="2.1 可视化"></a>2.1 可视化</h3><p>和制造业生产流程不一样，IT技术行业的工作内容一般是不可见的。比如开发人员说“忙不过来”的时候，到底有多少功能等待他开发才让他忙不过来？测试人员抱怨自己”测不过来“时，到底有多少功能需要测试的时候测试人员才会测不过来？这些都是不可见的。不可见就会造成整个开发过程中哪个环节受阻？哪个环节投入资源过剩？这些问题都不知道。</p><p>为了识别工作队列的流动、排队或者停止的情况，就要把工作可视化。可视化的方式有很多形式，比如看板或者Sprint计划版，甚至是物理的黑板表格等等。</p><p>看板能清晰地将工作可视化，从任务的创建时间到完成时间，可以推算出工作<a href="devops/data.md">前置时间</a>，通过提升管理任务的效率，加速任务从左到右的流动速度。通过可视化，项目经理和产品经理能列出各项工作的优先级，每次迭代就可以从优先级高的任务开始。</p><p>看板形式如下:<br>    <img src="/assets/img/kanban.jpg" alt="敏捷开发" width="400" ></p><h3 id="2-2-在制品限制"><a href="#2-2-在制品限制" class="headerlink" title="2.2 在制品限制"></a>2.2 在制品限制</h3><p>在制造业的日常工作，通常是由定期生成的生产计划决定的。计划会根据客户订单、交货日期、零件库存等条件，确定执行哪些任务。但是IT技术工作过程中会发现紧急、临时安插的任务数量，可能比计划进行任务数量还要多，这种临时的任务往往会打断工程师的思路，造成工程师在不同任务之间切换，而这种中断造成的代价往往是不可见的。</p><p>比如有个工程师为了开发一个计划中的功能，需要重构一段遗留代码，这段代码读了一半，还没理清逻辑。这时测试人员说有需要紧急修复的 Bug ，工程师就需要停止读代码，为了调试和修复这个 Bug ，开始读另一段代码。如果这时候产品经理又跑过来，说要加需求，然后开始描述，这时候该工程师又要切换思路，想一下这个需求能不能实现，实现要多久。在这时，工程师手上有 3 个任务，3 个任务都没有完成，假如最初工程师读遗留代码的时间花了 10 分钟，再读 5 分钟，就知道怎么改了。也就是在等 5 分钟，就有一项任务能被完成了，前置时间就变少了，而临时安插的 Bug 修复任务和新增的需求，则让前置时间变长了。这时候回过头来，可能又要花多 15 分钟，也就是原本 15 分钟能完成的任务，现在需要 30 分钟，工作效率大大降低。</p><p>在制品限制是影响前置时间的关键因素。如上看板，一般在制品限制的做法是在标题中添加在制品数量，比如”Selected (2)”。在开发时在制品往往来自带实现的需求或者待修复的缺陷。当某个工作队列的工作数量达到上限时，就要进制添加新卡片。通过限制在制品数量，还能发现开发流程中存在的阻碍，比如有的团队会发现测试团队经常没事干，因为要等待开发人员开发完成。又或者发现开发团队没事干，因为要等待产品经理把需求分析完成。当我们查明并且解决导致等待的原因后，我们就能够减少任务前置时间。</p><h3 id="2-3-任务拆分"><a href="#2-3-任务拆分" class="headerlink" title="2.3 任务拆分"></a>2.3 任务拆分</h3><p>建立快速的工作流的关键是通过小批量的模式完成工作。所以我们必须合理的拆分当前工作，避免出现大批量任务，因为大批量任务更容易造成拥堵，修改调整也相当困难，从而会增加前置时间，影响交付质量。</p><p>个关于著名高尔夫球手“老虎”伍兹的故事。高尔夫球手在打球的时候，可能会受到一些外界干扰。一般情况下还好，如果他已经开始挥杆，这时候受到了干扰，一般选手肯定是继续把杆挥下去，但通常的结果是打得不理想。而伍兹遇到这种情况，他会停下来，重新做挥杆的动作，保证了每一杆动作的标准。伍兹能停下来，固然是经过了大量的练习，但还有一个关键在于，对于别人而言，挥杆击球是一个动作，必须一气呵成。而对伍兹来说，这个动作是由若干小动作组成的，他只不过是刚好完成了某个小动作，而没有做下一个小动作而已。换句话说，大家同样都是完成一个原子操作，只不过，伍兹的原子操作比其他人的原子操作小得多。这个故事其实说的是任务分解。</p><p>敏捷开发中的迭代追求的是可及时调整中间过程，在需求变化的时候还能掌握变化，从而可以在短周期内经常性交付有价值的软件。在技术价值流中，单件流可以通过持续部署实现，每一个提交到版本控制系统的变更，都会进行集成、测试并部署到生产环境。</p><h3 id="2-4-减少交接"><a href="#2-4-减少交接" class="headerlink" title="2.4 减少交接"></a>2.4 减少交接</h3><p>代码在技术价值流流转的过程中，需要各个部门的协同才能完成，包括功能测试、集成测试、环境搭建、配置服务器、存储管理、网络、负载均衡设备和信息安全加固等工作。一项工作在团队之间交接时，需要大量的沟通、请求、委派、通知、协调，而且需要安排优先级、调度、消除冲突、测试和验证。</p><p>而且，在交接过程中往往会丢失某些关键信息，甚至造成重复的工作。比如开发人员让运维人员创建一个新的DB用来测试，运维人员可能在忙其他的，过了一会才帮忙创建一个DB，但是运维人员不知道这个DB是用来干什么的，也可能通过有效沟通不需要创建。为了减少类似问题，可以减少人为的交接工作，比如使用自动化操作，让开发人员自服务的平台等。</p><p>我们要减少工作在队列中的等待时间，减少非增值工作的时间，以增加工作的流动性。</p><h3 id="2-5-约束点"><a href="#2-5-约束点" class="headerlink" title="2.5 约束点"></a>2.5 约束点</h3><p>为了缩短前置时间、提高生产系统的吞吐量，我们要不断识别系统中的约束点，才能提高产能。《目标》一书中提到：在任何价值流中，总有一个流动方向、一个约束点（瓶颈），任何不针对该约束点做的优化都是徒劳的。</p><p>比如当测试部门的工作过量时，再对开发效率进行优化时，就会有更多的功能需要测试，导致优化的结果让总的产出效率下降了。</p><p>约束点的优化，应该遵循以下流程</p><p><strong>识别瓶颈 -&gt; 找出措施 -&gt; 协力解决 -&gt; 突破瓶颈 -&gt; 重复执行</strong></p><p>在DevOps的转型过程中，我们常常遇见以下约束点</p><ul><li>环境的搭建</li></ul><p>我们部署一套基础设施等待应用的部署需要多长时间？在过去如果是物理机，从采购、上架、配电、安装操作系统、配置网络等怎么也得1个多月以上。后来使用了虚拟机就大大缩短这一过程。现在最新的是使用容器，进一步缩短了时间，如果有一套k8s平台，那么环境的准备基本上可以忽略不计。</p><p>所以保证团队成员能通过自动化的方式创建环境，是解决<code>环境搭建</code>这个约束点的关键</p><ul><li>代码部署</li></ul><p>如果代码部署需要很长时间，比如每次部署都有很多手动操作，并涉及很多人员，那就无法实现按需部署。</p><p>对于实施运维人员最可气的就是一份几百页的部署文档！</p><p>解决这个约束点的关键是尽量自动化代码部署的过程，让每个开发人员，都可以按需自动化部署代码。比如使用持续集成工具Jenkins等</p><ul><li>测试<br>如果每次代码部署，都要两个星期的时间完成测试环境的准备和数据集的配置，手动执行所有回归测试也要 4 个星期的时间，那就无法实现按需部署。<br>解决办法是实现自动化测试，以便在安全、并行地执行部署的同时，让测试的速度跟上开发的速度。或者使用A\B测试、灰度发布等</li></ul><p>如果能突破上面 3 个约束点，那接下来可能遇到约束点就会是开发部门或产品部门</p><h3 id="2-5-消除浪费"><a href="#2-5-消除浪费" class="headerlink" title="2.5 消除浪费"></a>2.5 消除浪费</h3><p>丰田生产系统的先驱之一新乡重夫认为，浪费是企业增长的最大威胁，精益中对浪费的常用定义是：使用超出客户需求和客户愿意支付的范围内的任何材料或资源的行为。</p><p>新乡重夫定义了制造业中 7 种主要的浪费类型：库存、过量生产、过度加工、运输、等待、移动和缺陷。</p><p>浪费一次似乎有主动的含义，但是并不一定是工程师主动造成浪费。这是一种困境。</p><p>为了消除浪费往往通过以下类型</p><ul><li>半成品，项目中不应该存在长期的半成品，因为随着时间的推移，半成品必将失去价值</li><li>额外的工序，在交付过程中未带来价值的工作，比如下游工作中从来没有使用过的文档，不使用的输出结果</li><li>额外的功能,额外功能增加了功能测试、管理复杂度和工作量，额外功能包括在交付过程中构建的，组织或客户不需要的功能。比如花时间搞炫酷的功能</li><li>任务切换，一个人在多个任务和价值流中，必然会有任务的切换，切换会导致耗费额外的工作量和时间</li><li>等待,由于资源竞争，在工作是产生等待，减慢了流动速度</li><li>会议，不必要的会议会造成很大的浪费，因为人多的会议往往很难形成结论。</li><li>手动操作, 我们应该尽量避免手动的非标准的操作，理想的情况下，任何依赖运维团队手动完成的操作，都应该配置为能自动化且按需提供的，也就是自助服务。</li></ul><h2 id="3-反馈机制"><a href="#3-反馈机制" class="headerlink" title="3. 反馈机制"></a>3. 反馈机制</h2><p>Andon系统（也称“安灯”、“暗灯”，原为日语的音译，日语的意思为“灯光”、“灯笼”），最早起源于日本丰田汽车公司，用来实现“立即暂停制度”，以即时解决质量问题（而不是下线返修），达到持续高品质地生产汽车。</p><p>Andon工作流程主要包含：</p><ol><li>当操作者需要帮助、发现质量等与产品制造、质量有关的问题，他就拉下吊绳或用遥控器，激活Andon系统，该信息通过操作工位信号灯、Andon看板、广播将信息发布出去，提醒所有人注意。</li><li>班组长响应质量要求，与操作工一同确定问题。如果班组长可以解决问题，重新拉下吊绳，系统恢复正常。如果确定的问题必须向其它部门求助解决，则班组长通过设置在区域集中呼叫台进行呼叫，将信息类型、呼叫内容再次通过Andon看板、广播将信息发布出去，呼叫物料、质量、油漆、维修等前去处理问题。</li><li>如果在规定时间无法解决问题，则会停掉整个生产线，让所有人一起合作，直到解决问题为止。</li></ol><img src="/assets/img/andon.jpg" alt="敏捷开发" width="400" ><p>立即解决问题有以下好处</p><ul><li>减少修复成本，防止把问题带到下游，否则修复成本会加剧</li><li>避免引入错误，避免一个问题引发其他问题</li><li>避免重复损失，立刻解决问题防止同一个问题再次引发故障</li></ul><p>Cynefin框架描述了组织所处的环境。组织所处环境有五种类型，分别是简单、繁杂、复杂、混沌、失序。</p><p>简单是指，因果关系清晰可见的，决策往往为决策者可以采用的策略是：感知（sense）–分类（categorize）–响应（respond）。繁杂是因果关系明确，但需要专家研究和解决，决策策略为：感知（sense）–分析（analyze）–响应（respond）。复杂是指，因果关系可以被回溯，但不可能提前预知，解决方案慢慢浮出来，而不是一次性就明确的，所以需要不断摸索、不管试错，不断迭代,策略为:探索（probe）–感知（sense）–响应（respond）。混沌是因果关系不清晰，信息复杂多变，这个时候往往采取的策略为：行动（act）–感知（sense）–响应（respond）。失序就是黑洞，不清楚是不是出了问题，也不清楚问题属于哪个常见，各种冲突不断。</p><blockquote><p>反者道之动，当我们处于任何一种环境中都不知道骄傲，也不应该沮丧，因为随着知识的积累，我们可以从混乱到复杂再到繁杂再到简单顺时针移动；同样，偏见的积累、自满也会导致灾难的发生，产生简单到繁杂到复杂的逆时针移动。</p></blockquote><p>在IT工作过程中，我们大部分时候处在复杂这一状态中，所以我们的策略应为：密集立刻解决问题，而不是绕开问题，密集解决问题来促进学习。这种全民动员的做法与常规的管理方法不同，因为局部问题扰乱了整体的运营，但是这种方式让大家都能从问题中学习到知识。立刻解决问题，能防止记忆模糊和情况变化导致的关键信息丢失，关键信息对于修复复杂系统中的问题很重要。在复杂系统中，人员、流程、产品、地点和情况都存在很多意料之外、特殊的相互作用，随着时间的推移，往往很难再重现问题发生时的场景。</p><p>要想在技术价值流中建立快速反馈的机制，我们就要建立类似于安灯绳和全员响应的机制。</p><p>我们要树立一种文化，鼓励大家在发生问题时拉动安灯绳，无论是生产事故还是价值流的早期出现错误，这个行为是安全的。</p><p>比如当有人提交代码导致持续构建或测试失败，触发安灯绳时，我们就可以聚集在一起解决问题，停止开展任何新的工作，直到问题解决了为止。使用Andon绳机制，能让我们快速定位问题，避免出现更复杂的情况，导致问题因果关系变的模糊。</p><p>那么在实际工作中我们该如何做呢？</p><h3 id="3-1-在源头控制"><a href="#3-1-在源头控制" class="headerlink" title="3.1 在源头控制"></a>3.1 在源头控制</h3><p>对意外和事故的临时处理模式，我们可能在无意之中将本不安全的系统固化下来。在复杂的系统中，通过加入更多的审批流程，不断不会降低故障率，甚至会导致故障率的上升。因为在作出决策的地方往往原理执行工作的地方，导致审批流程的有效性下降。建立更多审批流程，不仅降低了决策质量，还增加了决策周期，削弱了因果关系之间的反馈强度，降低了工作人员在成功和失败中学习的能力。即使在一些较小的简单系统中，也存在这种情况，一般是因为清晰度和及时性不足，自上而下的官僚主义和控制系统变得无效，导致应该做事的人和实际做事的人存在巨大差异。</p><p>我们如何做到在源头控制质量</p><ul><li><p>避免不必要的等待<br>问题应该被立刻解决而不是等其流转到下游团队，比如测试团队、运维团队等</p></li><li><p>自动化</p></li></ul><p>需要其他团队帮忙完成一系列乏味、易出错和需要手动执行的任务，这些任务本该由需求方采用自动化的方式完成。比如测试人员需要开发人员才能打包，而不是通过 Jenkins 自己调整配置，自动打包。</p><ul><li>现场执行工作</li></ul><p>现场权限放开，不需要在远距离的领导做决策，因为在工作繁忙的领导批准时，往往出现领导在不了解工作情况和潜在影响的情况下，作出低质量的决策，又或者只是例行公事地盖章。</p><ul><li>避免无效的文档</li></ul><p>往往文档的歧义很大，更新也不及时，很可能在刚写完不久就已经过时，在必要的时候应该引入BDD（行为驱动开发)</p><p>在源头保证质量，意味着不论是质量控制还是安全责任，决策都要由执行工作的人作出，而不是依赖于高层领导的审批。根据同行评审评定变更，确保变更将按照设计运行，以自动化的方式进行质量和安全性检查。</p><h2 id="3-持续学习与实验原则"><a href="#3-持续学习与实验原则" class="headerlink" title="3.持续学习与实验原则"></a>3.持续学习与实验原则</h2><p>三步工作法中的第三部，就是一个螺旋上升的过程，当然这个过程得有组织文化的承托</p><h3 id="3-1-免责"><a href="#3-1-免责" class="headerlink" title="3.1 免责"></a>3.1 免责</h3><p>我们工作在复杂系统中，不可避免会经常出错。出现错误时，很多公司处理的方法是：Name , Blame , Shame。出现错误时是一种责备和追责为主的文化。这样会导致组织内形成一种恐惧感，如果团队成员都很恐惧，怕做错事受到责备，就会选择把小的问题隐藏掉，直到问题积累到一定程度最终灾难性的爆发。</p><p>Westrum模型提出组织文化的三种类型，分别是病态型组织、官僚型组织和生机型组织。</p><p><strong>病态型组织</strong>的特点是大量的恐惧和威胁，倾向于隐藏失败；</p><p><strong>官僚型组织</strong>的特点是严格的规则和流程，每个部门各扫门前雪；</p><p><strong>生机型组织</strong>是积极寻找和共享信息，责任共享，失败引发反思。</p><p>在DevOps的范畴里应该构建生机型的文化，引导一个免责的故障事后分析机制，让学习过程变成良性的循环。</p><p>2017年1月31号晚上GitLab发生了非常重大的事故，某个运维人员在凌晨删除了数据库，大概有6小时的数据丢失，包括合并请求、评论等，而且是永久性丢失。有707位用户、5037个项目受影响。</p><p>官方视频直播了恢复过程，他们主要做的是问题根因分析，采用了5W（五个为什么）的方法，分析问题并制定改进措施。但是官方也表态，造成问题的运维人员不会被解雇，而是被迫看一个很无聊的视频作为处罚。</p><p>这个例子反映出DevOps提倡的是免责的文化，重点并不在追责，而是转移到根因分析和对于恢复过程的改进，这是生机型组织的明显特征。</p><h3 id="3-2-改进与学习"><a href="#3-2-改进与学习" class="headerlink" title="3.2 改进与学习"></a>3.2 改进与学习</h3><p>Linkedin 2003年成立，当时核心业务运行在叫做Leo的Java单体应用上，多年发展后，2010年Leo外围有100个系统，问题是核心的Leo系统经常宕机，难以发布新代码。于是Linkedin做出一个决定，通过两个月时间，完全不做新的功能开发，完全聚焦在非功能需求上，将之前积累近十年的技术债务一次性偿还，相信这是一个非常艰难的决定。这个案例给我们的启示，比日常工作更重要的，是改进日常工作，我们要把改进融入到日常的工作中。</p><p>如果团队没有能力或意愿改进现有的工作流程，就会持续遭受眼前问题的折磨，而且问题带来的麻烦往往会与日俱增。</p><p>Mike Rother 在《丰田套路》一书中指出，就算我们不去优化工作的现状，流程也不会一成不变，混乱和无序的流程，会随着时间的推移持续恶化。在技术价值流中，为了防止灾难性事故的发生，团队陷于实施各种临时解决方案的工作中，没有时间完成有价值的工作。用临时方案解决问题的模式，往往会导致问题和技术债务的积累。在每个开发周期的间歇中预留一段时间，或让工程师通过自组团队的方式，解决他们感兴趣的问题。明确预留实践改善日常工作，包括偿还技术债、修复缺陷、重构、优化代码和环境等。</p><p>一旦改善在某个环节取得成功，就应该把他分享给全局，让更多的人从中收益，当然这种知识不能局限在文档层面，而是应该转化为显性知识，比如将其应用于实践，改善其他环节的工作等。</p><p>此外不同组织应该频繁分享和交流优化的方法，共同学习有效的改进。</p><h3 id="3-3-注入故障和弹性"><a href="#3-3-注入故障和弹性" class="headerlink" title="3.3 注入故障和弹性"></a>3.3 注入故障和弹性</h3><p>丰田的一个顶级供货商有两条生产线，但是在Slow Day的时候会把所有的制造放在一条生产线上，用来实验增大压力、扩充容量是否会导致失败，增强反脆弱的能力。</p><p>同样在技术价值流中，也有成为Game Day的演习，模拟大规模失败，检查系统稳定性。Netflix的Chaos Monkey工具，会在生产环境随机杀进程，用来确保整个系统有强大的恢复能力。Martin Fowler提出“If it hurts，do it more often”。即如果某个事情让你感到痛苦，那么频繁去做，那么你将不会感觉到痛苦。</p><p>在技术团队中，我们要不断给技术团队施压，来试探出瓶颈，然后优化瓶颈。持续不断的实验，即使当前比较稳定，我们也要在极限的边缘和故障的边缘游走，从而能提高团队的韧性，使真正出现故障或者灾难时，我们能像日常工作那样处理。</p>]]></content>
    
    <summary type="html">
    
      DevOps落地的三步工作法
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="devops" scheme="http://huaqiang.art/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>容器与云原生</title>
    <link href="http://huaqiang.art/2020/05/07/containertec/"/>
    <id>http://huaqiang.art/2020/05/07/containertec/</id>
    <published>2020-05-07T06:24:20.000Z</published>
    <updated>2020-05-07T06:41:46.421Z</updated>
    
    <content type="html"><![CDATA[<p>此文档为给客户培训时是整理文档内容</p><a id="more"></a><h2 id="容器技术交流"><a href="#容器技术交流" class="headerlink" title="容器技术交流"></a>容器技术交流</h2><h3 id="虚拟机和容器"><a href="#虚拟机和容器" class="headerlink" title="虚拟机和容器"></a>虚拟机和容器</h3><p>虚拟机：虚拟机是通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中具有完整计算机系统。</p><p><img src="/assets/img/image-20200312141328342.png" alt="image-20200312141328342"></p><p>容器：是一个允许在资源隔离过程中、运行应用程序和其依赖的、轻量级的操作系统级别的虚拟化技术</p><p><img src="/assets/img/image-20200312141335274.png" alt="image-20200312141335274"></p><h4 id="为什么使用容器"><a href="#为什么使用容器" class="headerlink" title="为什么使用容器"></a>为什么使用容器</h4><ul><li><p>高效的利用系统资源</p></li><li><p>快速的启动时间</p></li><li><p>一致的运行环境</p></li><li><p>持续交付和持续部署</p></li><li><p>方便的迁移</p></li><li><p>方便的维护和扩展</p></li></ul><h4 id="容器和虚拟机对比"><a href="#容器和虚拟机对比" class="headerlink" title="容器和虚拟机对比"></a>容器和虚拟机对比</h4><table><thead><tr><th><strong>特性</strong></th><th><strong>容器</strong></th><th><strong>虚拟化</strong></th></tr></thead><tbody><tr><td>启动</td><td>秒级别</td><td>分钟级别</td></tr><tr><td>存储使用</td><td>一般为  MB</td><td>一般为  GB</td></tr><tr><td>性能</td><td>接近原生</td><td>弱于</td></tr><tr><td>系统支持量</td><td>单机最大可达数千个</td><td>一般几十个</td></tr></tbody></table><ul><li><input disabled="" type="checkbox"> 容器为当前CI/CD、DevOps、serverless等先进技术理念提供了更好的技术土壤</li></ul><h4 id="docker三要素"><a href="#docker三要素" class="headerlink" title="docker三要素"></a>docker三要素</h4><p><img src="/assets/img/image-20200312162117664.png" alt="image-20200312162117664"></p><p>repository：Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库</p><p>镜像：Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。unionFS</p><p>容器：Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。</p><h4 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h4><p>数据卷是一个可以供一个或者多个容器使用的特殊目录，它绕过UFS，可以提供很多特性。</p><ul><li><p>数据卷可以在容器之间共享和重用</p></li><li><p>对数据卷的修改会立马生效</p></li><li><p>对数据卷的更新，不会影响镜像</p></li><li><p>数据卷会一致存在，即使容器被删除</p></li></ul><h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h4><p>docker有多种网络模式</p><ul><li><p>host 模式，使用 –net=host 指定。</p></li><li><p>container 模式，使用 –net=container:NAMEorID 指定。</p></li><li><p>none 模式，使用 –net=none 指定。</p></li><li><p>bridge 模式，使用 –net=bridge 指定，默认设置。</p></li></ul><p><img src="/assets/img/image-20200312162334994.png" alt="image-20200312162334994"></p><h3 id="容器编排"><a href="#容器编排" class="headerlink" title="容器编排"></a>容器编排</h3><p>单机运行容器无法体现出容器的优势，只有在编排时才能体现出容器的魅力。</p><h4 id="docker-compose"><a href="#docker-compose" class="headerlink" title="docker-compose"></a>docker-compose</h4><h5 id="docker-三剑客"><a href="#docker-三剑客" class="headerlink" title="docker 三剑客"></a>docker 三剑客</h5><p><del>Machine</del> </p><p><del>Docker Swarm</del></p><p>compose</p><p>compose 项目是docker官方开源项目，负责实现对docker容器集群的快速编排，其定位是定义和运行多个docker容器的应用。</p><h5 id="两个核心概念"><a href="#两个核心概念" class="headerlink" title="两个核心概念"></a>两个核心概念</h5><p>服务（service): 一个应用容器，实际上可以包括若干个运行相同镜像的容器实例。</p><p>项目(project):由一组关联的应用容器组成的完成业务单元，在docker-comose.yml中定义</p><p>docker-compose.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;1&#39;</span><br><span class="line">services:</span><br><span class="line">  httpd-test:</span><br><span class="line">    image: httpd:2.4</span><br><span class="line">  httpd-test-2:</span><br><span class="line">    image: httpd:2.4</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;&#x2F;data:&#x2F;var&#x2F;www&#x2F;html&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;80:80&quot;</span><br><span class="line">  httpd-test-3:</span><br><span class="line">    image: httpd:2.4</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;&#x2F;data:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:80&quot;</span><br></pre></td></tr></table></figure><h4 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h4><p>kubernetes是谷歌开源的容器集群管理系统，是谷歌多年大规模容器管理技术Borg的开源版本，也是CNCF最重要的项目之一。</p><h5 id="k8s功能"><a href="#k8s功能" class="headerlink" title="k8s功能"></a>k8s功能</h5><ul><li><p>基于容器的应用部署、维护、滚动升级。</p></li><li><p>跨机器和跨地区的集群调度</p></li><li><p>负载均衡和服务发现</p></li><li><p>自动伸缩</p></li><li><p>无状态服务和有状态服务</p></li><li><p>插件机制保证扩展性</p></li></ul><h5 id="k8s整体架构"><a href="#k8s整体架构" class="headerlink" title="k8s整体架构"></a>k8s整体架构</h5><p><img src="/assets/img/image-20200312163332129.png" alt="image-20200312163332129"></p><ul><li><p><strong>核心组件</strong></p></li><li><ul><li>etcd保存了整个集群的状态；</li></ul></li><li><ul><li>apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；</li></ul></li><li><ul><li>controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li></ul></li><li><ul><li>scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；</li></ul></li><li><ul><li>kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；</li></ul></li><li><ul><li>Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；</li></ul></li></ul><p>kube-proxy负责为Service提供cluster内部的服务发现和负载均衡</p><ul><li><strong>Addons</strong></li></ul><p>•kube-dns负责为整个集群提供DNS服务</p><p>•Ingress Controller为服务提供外网入口</p><p>•Heapster提供资源监控</p><p>•Dashboard提供GUI</p><p>•Federation提供跨可用区的集群</p><p>•Fluentd-elasticsearch提供集群日志采集、存储与查询</p><h5 id="pod调度过程"><a href="#pod调度过程" class="headerlink" title="pod调度过程"></a>pod调度过程</h5><p><img src="/assets/img/image-20200312163704823.png" alt="image-20200312163704823"></p><h5 id="k8s资源对象"><a href="#k8s资源对象" class="headerlink" title="k8s资源对象"></a>k8s资源对象</h5><ul><li>Pod</li></ul><p>Kubernetes创建或部署的最小/最简单的基本单位，一个Pod代表集群上正在运行的一个进程。一个Pod封装一个应用容器（也可以有多个容器），存储资源、一个独立的网络IP以及管理控制容器运行方式的策略选项。Pod代表部署的一个单位：Kubernetes中单个应用的实例，它可能由单个容器或多个容器共享组成的资源。</p><ul><li>计算资源</li></ul><p>包含deployment,daemonset,statefulset,job等多种资源，用于pod上层的调度，编排pod的核心资源</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source: tomcat/templates/appsrv.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    chart:</span> <span class="string">tomcat-0.4.1</span></span><br><span class="line"><span class="attr">    release:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    heritage:</span> <span class="string">Tiller</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">      release:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">        release:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">app-volume</span></span><br><span class="line"><span class="attr">          emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"><span class="attr">      initContainers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">war</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">ananwaresystems/webarchive:1.0</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"sh"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">"cp /*.war /app"</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">app-volume</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/app</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">          image:</span> <span class="attr">tomcat:7.0</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">app-volume</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/usr/local/tomcat/webapps</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">              hostPort:</span> <span class="number">8009</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/sample</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          readinessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/sample</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line">            <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><ul><li>存储资源</li></ul><p>PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。PV是集群资源。</p><p>PersistentVolumeClaim（PVC）是用户存储的请求。它类似于pod。Pod消耗节点资源，PVC消耗存储资源。</p><p>StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。</p><p>secret和configmap可以理解为特殊的存储卷，但是它们不是给Pod提供存储功能的，而是提供了从集群外部向集群内部的应用注入配置信息的功能。</p><ul><li>服务资源</li></ul><p>service：一个 Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。 这一组 Pod 能够被 Service 访问到，可以简单理解为k8s内部负载均衡，</p><p><img src="/assets/img/image-20200312164435191.png" alt="image-20200312164435191"></p><p>ingress：是在外面访问和service中间一层，可以简单理解为k8s中7层负载均衡</p><p><img src="/assets/img/image-20200312164447625.png" alt="image-20200312164447625"></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source: tomcat/templates/appsrv-svc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    chart:</span> <span class="string">tomcat-0.4.1</span></span><br><span class="line"><span class="attr">    release:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    heritage:</span> <span class="string">Tiller</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">      protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">tomcat</span></span><br><span class="line"><span class="attr">    release:</span> <span class="string">tomcat</span></span><br></pre></td></tr></table></figure><h3 id="云原生"><a href="#云原生" class="headerlink" title="云原生"></a>云原生</h3><h4 id="服务网格"><a href="#服务网格" class="headerlink" title="服务网格"></a>服务网格</h4><p>又称为service mesh，通过sidecar的形式解决微服务之间服务控制，服务发现，负载均衡等问题。</p><p><img src="/assets/img/image-20200312164957832.png" alt="image-20200312164957832"></p><p><img src="/assets/img/image-20200312165151749.png" alt="image-20200312165151749"></p><h4 id="云原生概念"><a href="#云原生概念" class="headerlink" title="云原生概念"></a>云原生概念</h4><p>Pivotal定义</p><ul><li><p>符合12因素应用</p></li><li><p>面向微服务架构</p></li><li><p>自服务敏捷架构</p></li><li><p>基于API的协作</p></li><li><p>抗脆弱性</p></li></ul><blockquote><p>12因素</p><ol><li>一份基准代码，多份部署</li><li>显示声明依赖关系</li><li>在环境中存储配置</li><li>把后端服务当作附加资源</li><li>严格分离构建和运行</li><li>以一个或者多个无状态进程运行应用</li><li>通过端口绑定提供服务</li><li>通过进程模型进行扩展</li><li>快速启动和优雅终止可最大化健壮性</li><li>尽可能的保持开发、预发布、线上环境相同</li><li>把日志当作时间流</li><li>后台管理任务当作一次性进程运行</li></ol></blockquote><p>CNCF定义</p><ul><li><p>应用容器化</p></li><li><p>面向微服务架构</p></li><li><p>应用支持容器的编排调度</p></li></ul><p>CNCF后期定义</p><p>云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。</p><ul><li><input checked="" disabled="" type="checkbox"> 云原生应用就是为了在云上运行而开发的应用。</li></ul><p><img src="/assets/img/image-20200312170109097.png" alt="image-20200312170109097"></p>]]></content>
    
    <summary type="html">
    
      容积技术基本是当前相当火热的技术，但是容器技术到底是什么，和微服务、Devops到底有什么关系呢？
    
    </summary>
    
    
      <category term="devops" scheme="http://huaqiang.art/categories/devops/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="devops" scheme="http://huaqiang.art/tags/devops/"/>
    
      <category term="容器云" scheme="http://huaqiang.art/tags/%E5%AE%B9%E5%99%A8%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>helm 安装gitlab</title>
    <link href="http://huaqiang.art/2020/04/13/gitlab/"/>
    <id>http://huaqiang.art/2020/04/13/gitlab/</id>
    <published>2020-04-13T13:27:20.000Z</published>
    <updated>2020-04-13T13:33:01.638Z</updated>
    
    <content type="html"><![CDATA[<p>helm 一键安装gitlab在k8s之上。由于官网chart需要下载国外的组件，故此处使用阿里镜像来满足国内在k8s之上的安装</p><a id="more"></a><h4 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$kubectl</span> create ns nginx-ingress</span><br></pre></td></tr></table></figure><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">helm  install gitlab-pro gitlab/gitlab \</span><br><span class="line">--namespace gitlab \</span><br><span class="line">--version 3.2.1 \</span><br><span class="line">--<span class="built_in">set</span> global.edition=ce \</span><br><span class="line">--<span class="built_in">set</span> global.hosts.domain=gitlab.devops.nari \</span><br><span class="line">--<span class="built_in">set</span> certmanager.install=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> global.ingress.configureCertmanager=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> upgradeCheck.enabled=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> certmanager.createCustomResource=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> nginx-ingress.enabled=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> certmanager.install=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> prometheus.install=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> gitlab-runner.install=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> gitlab.registry.enabled=<span class="literal">false</span> \</span><br><span class="line">--<span class="built_in">set</span> shared-secrets.selfsign.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/cfssl-self-sign \</span><br><span class="line">--<span class="built_in">set</span> global.kubectl.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/kubectl \</span><br><span class="line">--<span class="built_in">set</span> global.certificates.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/alpine-certificates \</span><br><span class="line">--<span class="built_in">set</span> gitlab.global.communityImages.sidekiq.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-sidekiq-ce \</span><br><span class="line">--<span class="built_in">set</span> gitlab.global.communityImages.task-runner.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-task-runner-ce \</span><br><span class="line">--<span class="built_in">set</span> gitlab.global.communityImages.migrations.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-task-runner-ce \</span><br><span class="line">--<span class="built_in">set</span> gitlab.global.communityImages.unicorn.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-webservice-ce \</span><br><span class="line">--<span class="built_in">set</span> gitlab.global.communityImages.unicorn.workhorse.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-workhorse-ce \</span><br><span class="line">--<span class="built_in">set</span> gitlab.gitaly.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitaly \</span><br><span class="line">--<span class="built_in">set</span> gitlab.gitlab-shell.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-shell \</span><br><span class="line">--<span class="built_in">set</span> gitlab.gitlab-exporter.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-exporter \</span><br><span class="line">--<span class="built_in">set</span> registry.image.repository=registry.cn-qingdao.aliyuncs.com/imageofout/gitlab-container-registry</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      在k8s上安装gitlab，使用helm 安装gitlab在官网上需要翻墙，此处设置相关参数使之不翻墙即可安装。
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="gitlab" scheme="http://huaqiang.art/tags/gitlab/"/>
    
      <category term="helm" scheme="http://huaqiang.art/tags/helm/"/>
    
  </entry>
  
  <entry>
    <title>helm安装openldap</title>
    <link href="http://huaqiang.art/2020/04/01/openldap/"/>
    <id>http://huaqiang.art/2020/04/01/openldap/</id>
    <published>2020-04-01T14:54:12.000Z</published>
    <updated>2020-04-04T11:00:07.772Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>OpenLDAP是轻型目录访问协议（Lightweight Directory Access Protocol，LDAP）的自由和开源的实现，在其OpenLDAP许可证下发行，并已经被包含在众多流行的Linux发行版中。如果你理解微软的AD，那么他将提供和AD类似的功能。</p><a id="more"></a><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><img src="/assets/img/openldap.jpg" width="80%"><h3 id="openLDAP-server配置"><a href="#openLDAP-server配置" class="headerlink" title="openLDAP server配置"></a>openLDAP server配置</h3><h4 id="cert-manager生成证书"><a href="#cert-manager生成证书" class="headerlink" title="cert-manager生成证书"></a>cert-manager生成证书</h4><p>CA证书生成过程可以参考cert-manager中CA部分,此处直接通过CA的issuer创建服务器所需的certificates，其中<code>secretName</code>后面会用到，<code>commonName</code>可以写ldap server的域名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cat openldap-server-cert.yaml</span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">  name: hq-opnldap1</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  secretName: hq-openldap1</span><br><span class="line">  issuerRef:</span><br><span class="line">    name: ca-issuer</span><br><span class="line">    kind: Issuer</span><br><span class="line">  commonName: hqopenldap.com</span><br><span class="line">$ kubectl apply -f openldap-server-cert.yaml</span><br></pre></td></tr></table></figure><h4 id="helm安装openldap"><a href="#helm安装openldap" class="headerlink" title="helm安装openldap"></a>helm安装openldap</h4><ul><li>检查是否包含openldap的helm仓库，仓库地址为：<a href="http://mirror.azure.cn/kubernetes/charts/">http://mirror.azure.cn/kubernetes/charts/</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo list</span><br><span class="line">NAME         URL                                                                      </span><br><span class="line">stable       http://mirror.azure.cn/kubernetes/charts/                                </span><br><span class="line"><span class="built_in">local</span>        http://127.0.0.1:8879/charts                                             </span><br><span class="line">ali-incubatorhttps://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/</span><br><span class="line">ali-stable   https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts                   </span><br><span class="line">jetstack     https://charts.jetstack.io</span><br></pre></td></tr></table></figure><blockquote><p>注意：若没有则添加仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add stable http://mirror.azure.cn/kubernetes/charts/</span><br></pre></td></tr></table></figure></blockquote><ul><li>查找openldap chart，并下载到本地</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ helm search openldap</span><br><span class="line">NAME           CHART VERSIONAPP VERSIONDESCRIPTION                      </span><br><span class="line">stable/openldap1.2.3        2.4.48     Community developed LDAP software</span><br><span class="line">$ helm fetch stable/openldap</span><br><span class="line"><span class="comment"># 解压文件</span></span><br><span class="line">$  tar -xzvf openldap-1.2.3.tgz</span><br></pre></td></tr></table></figure><ul><li>修改chart中的values.yaml文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> openldap</span><br><span class="line">$ cat values.yaml |grep -v <span class="string">'^ *#'</span>|grep -v <span class="string">'^$'</span></span><br><span class="line">replicaCount: 1</span><br><span class="line">strategy: &#123;&#125;</span><br><span class="line">image:</span><br><span class="line">  repository: osixia/openldap</span><br><span class="line">  tag: 1.2.4</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">existingSecret: <span class="string">""</span></span><br><span class="line">tls:</span><br><span class="line">  enabled: <span class="literal">true</span></span><br><span class="line">  secret: <span class="string">"hq-openldap1"</span>  <span class="comment"># The name of a kubernetes.io/tls type secret to use for TLS</span></span><br><span class="line">  CA:</span><br><span class="line">    enabled: <span class="literal">false</span></span><br><span class="line">    secret: <span class="string">"ca-key-pair"</span>  <span class="comment"># The name of a generic secret to use for custom CA certificate (ca.crt)</span></span><br><span class="line">extraLabels: &#123;&#125;</span><br><span class="line">podAnnotations: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  clusterIP: <span class="string">""</span></span><br><span class="line">  ldapPort: 389</span><br><span class="line">  sslLdapPort: 636  <span class="comment"># Only used if tls.enabled is true</span></span><br><span class="line">  externalIPs: []</span><br><span class="line">  loadBalancerIP: <span class="string">""</span></span><br><span class="line">  loadBalancerSourceRanges: []</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">env:</span><br><span class="line">  LDAP_ORGANISATION: <span class="string">"Example Inc."</span></span><br><span class="line">  LDAP_DOMAIN: <span class="string">"stopenldap.com"</span></span><br><span class="line">  LDAP_BACKEND: <span class="string">"hdb"</span></span><br><span class="line">  LDAP_TLS: <span class="string">"true"</span></span><br><span class="line">  LDAP_TLS_ENFORCE: <span class="string">"false"</span></span><br><span class="line">  LDAP_REMOVE_CONFIG_AFTER_SETUP: <span class="string">"true"</span></span><br><span class="line">adminPassword: admin</span><br><span class="line">configPassword: config</span><br><span class="line">persistence:</span><br><span class="line">  enabled: <span class="literal">true</span></span><br><span class="line">  accessMode: ReadWriteOnce</span><br><span class="line">  size: 2Gi</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">initResources: &#123;&#125;</span><br><span class="line">nodeSelector: </span><br><span class="line">  run: openldap</span><br><span class="line">tolerations: []</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line"><span class="built_in">test</span>:</span><br><span class="line">  enabled: <span class="literal">true</span></span><br><span class="line">  image:</span><br><span class="line">    repository: dduportal/bats</span><br><span class="line">    tag: 0.4.0</span><br></pre></td></tr></table></figure><p>主要修改的地方有：1.<code>tls.enabled: true</code>     启动tls认证</p><p>​                                   2.<code>tls.secret: hq-openldap1</code>   tls认证证书的secret，为cert-manager生成<code>secertNAME</code></p><p>​                                   3.<code>env.LDAP_DOMAIN: &quot;stopenldap.com&quot;</code> ldap DN名称</p><p>​                                   4.<code>adminPassword: admin</code>  admin的密码</p><p>​                                   5.<code>configPassword: config</code>  config的密码</p><p>​                                   6.<code>persistence.enabled:true</code>  设置持久化存储，默认时候default storageclass</p><p>​                                   7. <code>persistence.size:2G</code> 持久化存储大小</p><ul><li>安装</li></ul><p>在当前目录下执行helm install安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name hqopenldap --namespace default .</span><br></pre></td></tr></table></figure><blockquote><p><code>* 快速安装 *</code><br>如果不想本地保存chart，而是直接安装则可执行以下命令,其中env.LDAP_TLS_VERIFY_CLIENT=try表示client可以没有证书。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;$ helm install --name hqopenldap \</span><br><span class="line">&gt;--namespace default \</span><br><span class="line">&gt;--<span class="built_in">set</span> tls.enabled=<span class="literal">true</span> \</span><br><span class="line">&gt;--<span class="built_in">set</span> tls.secret=hq-openldap1 \</span><br><span class="line">&gt;--<span class="built_in">set</span> env.LDAP_DOMAIN=stopenldap.com \</span><br><span class="line">&gt;--<span class="built_in">set</span> adminPassword=admin \</span><br><span class="line">&gt;--<span class="built_in">set</span> configPassword=config \</span><br><span class="line">&gt;--<span class="built_in">set</span> persistence.enabled=<span class="literal">true</span> \</span><br><span class="line">&gt;--<span class="built_in">set</span> persistence.size=2Gi \</span><br><span class="line">&gt;--<span class="built_in">set</span> env.LDAP_TLS_VERIFY_CLIENT=try \</span><br><span class="line">&gt;stable/openldap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">检查是否启动成功</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">$ kubectl get pod -l app=openldap</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">hqopenldap-85d59d49b4-6jl7c   1/1     Running   0          138m </span><br></pre></td></tr></table></figure></blockquote><h3 id="客户端测试"><a href="#客户端测试" class="headerlink" title="客户端测试"></a>客户端测试</h3><h4 id="签发客户端所需的证书和私钥"><a href="#签发客户端所需的证书和私钥" class="headerlink" title="签发客户端所需的证书和私钥"></a>签发客户端所需的证书和私钥</h4><p>此处所使用的CA issuer和ldap server中所使用的CA需要为同一个,<code>commonName</code>可以自行定义，不同客户取名不同</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat client-cert.yaml</span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">  name: openldap-client1</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  secretName: client-test1</span><br><span class="line">  issuerRef:</span><br><span class="line">    name: ca-issuer</span><br><span class="line">    kind: Issuer</span><br><span class="line">  commonName: ldapadmin</span><br></pre></td></tr></table></figure><h4 id="导出证书文件"><a href="#导出证书文件" class="headerlink" title="导出证书文件"></a>导出证书文件</h4><p>由于测试是使用本地安装的LDAP Admin Tool，所有需要把k8s中生成的证书文件拿下来</p><ul><li>导出证书</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get secret/client-test1 -o jsonpath=<span class="string">"&#123;['data']['tls\.crt']&#125;"</span>|base64 --decode &gt; ldap.crt</span><br></pre></td></tr></table></figure><ul><li>导出私钥</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get secret/client-test1 -o jsonpath=<span class="string">"&#123;['data']['tls\.key']&#125;"</span>|base64 --decode &gt; ldap.key</span><br></pre></td></tr></table></figure><blockquote><p>注意: 由于LDAP Admin Tool使用的私钥为pkcs#8，所以需要将私钥转换下</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openssl pkcs8 -topk8 -inform PEM -<span class="keyword">in</span> ldap.key -outform pem -nocrypt -out ldap.pem</span><br></pre></td></tr></table></figure><h4 id="LDAP-Admin-Tool连接"><a href="#LDAP-Admin-Tool连接" class="headerlink" title="LDAP Admin Tool连接"></a>LDAP Admin Tool连接</h4><ul><li>导入证书和私钥</li></ul><p><code>Security</code>-&gt;<code>manage client certificates</code>-&gt;<code>Add Certificate</code>   导入证书</p><p><code>Security</code>-&gt;<code>manage client certificates</code>-&gt;鼠标选中证书-<code>Set Private Key</code> 导入私钥</p><ul><li>连接LDAP server</li></ul><p>连接信息中：Base DN: dc=stopenldap,dc=com    参照helm <code>values.yaml</code>中<code>env.LDAP_DOMAIN</code></p><p>​                       User DN: cn=admin,dc=stopenldap,dc=com 参照helm <code>values.yaml</code>中<code>env.LDAP_DOMAIN</code></p><p>​                       Port: 636</p><p>​                       Password: admin       参照helm <code>values.yaml</code>中<code>adminPassword</code></p><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>ldap server启动TLS时，默认需要验证client端的证书，若不想给客户端签发证书,可通过设置server端不验证证书即可，可以通过添加values.yaml中的一个参数设置:<code>env.LDAP_TLS_VERIFY_CLIENT: &quot;try&quot;</code></p><blockquote><p>never：不验证客户端证书。</p><p>allow：检查客户端证书，没有证书或证书错误，都允许连接。</p><p>try：检查客户端证书，没有证书（允许连接），证书错误（终止连接）。</p><p>demand | hard | true：检查客户端证书，没有证书或证书错误都将立即终止连接。</p><p>helm 安装的openldap默认为demand。</p></blockquote>]]></content>
    
    <summary type="html">
    
      通过certmanager管理openldap证书，开启openLDAP TLS模式
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="openLDAP" scheme="http://huaqiang.art/tags/openLDAP/"/>
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="cert-manager" scheme="http://huaqiang.art/tags/cert-manager/"/>
    
  </entry>
  
  <entry>
    <title>k8s证书管理cert-manager</title>
    <link href="http://huaqiang.art/2020/03/28/cert-manager/"/>
    <id>http://huaqiang.art/2020/03/28/cert-manager/</id>
    <published>2020-03-28T11:32:22.000Z</published>
    <updated>2020-04-04T09:53:20.008Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>cert-manager是kubernetes原生证书解决方案，可以通过多种机构颁发证书，证书最后以secret的形式存储在kubernetes中。可以支持Let’s encrypt，self signed，CA等。</p><a id="more"></a><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><img src="/assets/img/cert-manager.jpg" width="75%"><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="创建资源"><a href="#创建资源" class="headerlink" title="创建资源"></a>创建资源</h4><ul><li>创建crd</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Kubernetes 1.15+</span></span><br><span class="line">$ kubectl apply --validate=<span class="literal">false</span> -f https://github.com/jetstack/cert-manager/releases/download/v0.14.0/cert-manager.crds.yaml</span><br></pre></td></tr></table></figure><ul><li>helm安装</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加helm repo</span></span><br><span class="line">$ helm repo add jetstack https://charts.jetstack.io</span><br><span class="line"></span><br><span class="line"><span class="comment"># Helm v3+</span></span><br><span class="line">$ helm install \</span><br><span class="line">  cert-manager jetstack/cert-manager \</span><br><span class="line">  --namespace cert-manager \</span><br><span class="line">  --version v0.14.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Helm v2</span></span><br><span class="line">$ helm install \</span><br><span class="line">  --name cert-manager \</span><br><span class="line">  --namespace cert-manager \</span><br><span class="line">  --version v0.14.0 \</span><br><span class="line">  jetstack/cert-manager</span><br></pre></td></tr></table></figure><h4 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n cert-manager</span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">cert-manager-845c74ff94-5mdkk              1/1     Running   0          5h15m</span><br><span class="line">cert-manager-cainjector-648dd444df-h7jh5   1/1     Running   0          5h15m</span><br><span class="line">cert-manager-webhook-6dbcd48f9c-shh7z      1/1     Running   0          5h15m</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><h4 id="CA"><a href="#CA" class="headerlink" title="CA"></a>CA</h4><p>手动创建根证书和私钥，或者从其他地方获取根证书及私钥。将证书存储在k8s的secret中，用来颁发证书，可以用内部的CA证书信任生成的签名证书。</p><h5 id="创建根证书"><a href="#创建根证书" class="headerlink" title="创建根证书"></a>创建根证书</h5><ul><li>openssl生成密钥及证书</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ openssl genrsa -out ca.key 2048</span><br><span class="line">$ openssl req -x509 -new -nodes -key ca.key -subj <span class="string">"/CN=hqtest"</span> -days 3650 -reqexts v3_req -extensions v3_ca -out ca.crt</span><br></pre></td></tr></table></figure><ul><li>导入k8s的secret</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create secret tls ca-key-pair \</span><br><span class="line">   --cert=ca.crt \</span><br><span class="line">   --key=ca.key \</span><br><span class="line">   --namespace=default</span><br></pre></td></tr></table></figure><h5 id="创建签发机构"><a href="#创建签发机构" class="headerlink" title="创建签发机构"></a>创建签发机构</h5><ul><li><p>创建issuer,</p><p>Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书,类似k8s中role和clusterrole</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat issuer.yaml </span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Issuer</span><br><span class="line">metadata:</span><br><span class="line">  name: ca-issuer</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  ca:</span><br><span class="line">    secretName: ca-key-pair</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f issuer.yaml</span><br></pre></td></tr></table></figure><h5 id="创建证书"><a href="#创建证书" class="headerlink" title="创建证书"></a>创建证书</h5><ul><li><p>创建certtificate</p><p>cert-manager 给提供了 Certificate 这个用于生成证书的自定义资源对象，但它必须在某个namespace下</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat certs.yaml </span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">  name: example-com</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  secretName: example-com-tls</span><br><span class="line">  issuerRef:</span><br><span class="line">    name: ca-issuer</span><br><span class="line">    kind: Issuer</span><br><span class="line">  commonName: ning.com</span><br><span class="line">  organization:</span><br><span class="line">  - CA</span><br><span class="line">  dnsNames:</span><br><span class="line">  - ning.com</span><br><span class="line">  - nginx.ning.com</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f certs.yaml</span><br></pre></td></tr></table></figure><h5 id="ingress使用证书"><a href="#ingress使用证书" class="headerlink" title="ingress使用证书"></a>ingress使用证书</h5><ul><li>创建一个nginx例子</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">$ cat nginx.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: my-nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    name: http</span><br><span class="line">  selector:</span><br><span class="line">    run: my-nginx</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">"nginx"</span></span><br><span class="line">    kubernetes.io/tls-acme: <span class="string">"true"</span></span><br><span class="line">    certmanager.k8s.io/issuer: <span class="string">"ca-issuer"</span></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: nginx.ning.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: my-nginx</span><br><span class="line">          servicePort: 80</span><br><span class="line">        path: /</span><br><span class="line">  tls:</span><br><span class="line">  - secretName: nginx-secret</span><br><span class="line">    hosts:</span><br><span class="line">    - nginx.ning.com</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx.yaml</span><br></pre></td></tr></table></figure><h4 id="自签名证书"><a href="#自签名证书" class="headerlink" title="自签名证书"></a>自签名证书</h4><h5 id="创建自签名证书Issuer"><a href="#创建自签名证书Issuer" class="headerlink" title="创建自签名证书Issuer"></a>创建自签名证书Issuer</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cat cert-resource.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: cert-manager-test</span><br><span class="line">---</span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Issuer</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-selfsigned</span><br><span class="line">  namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">  selfSigned: &#123;&#125;</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f cert-resource.yaml</span><br></pre></td></tr></table></figure><h5 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a><span id="jump">生成证书</span></h5><ul><li>手动生成</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat certificate-example-com.yaml</span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">  name: selfsigned-cert</span><br><span class="line">  namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">  dnsNames:</span><br><span class="line">    - example.com</span><br><span class="line">  secretName: selfsigned-cert-tls</span><br><span class="line">  issuerRef:</span><br><span class="line">    name: <span class="built_in">test</span>-selfsigned</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f certificate-example-com.yaml</span><br></pre></td></tr></table></figure><ul><li>自动生成</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 selfsigne]<span class="comment"># cat nginx-self.yaml </span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-deploy</span><br><span class="line">  namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: myapp</span><br><span class="line">      release: canary</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: myapp</span><br><span class="line">        release: canary</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: myapp</span><br><span class="line">        image: ikubernetes/myapp:v1</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp</span><br><span class="line">  namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: myapp</span><br><span class="line">    release: canary</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80 </span><br><span class="line">    targetPort: 80 <span class="comment"># pod port</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-myapp</span><br><span class="line">  namespace: cert-manager-test</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: nginx</span><br><span class="line">    cert-manager.io/issuer: <span class="built_in">test</span>-selfsigned</span><br><span class="line">    kubernetes.io/tls-acme: <span class="string">"true"</span></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: myapp.self.tech</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: myapp</span><br><span class="line">          servicePort: 80</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - myapp.self.tech</span><br><span class="line">    secretName: myapp</span><br></pre></td></tr></table></figure><p>​    创建资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx-self.yaml</span><br></pre></td></tr></table></figure><blockquote><p>注意: 1. 自动创建证书中，标记ingress参数metadata.annotations.cert-manager.io/issuer: test-selfsigned,test-selfsigned为issuer名称</p><pre><code>2. 若使用clusterissuer则参数为 metadata.annotations.cert-manager.io/cluster-issuer</code></pre></blockquote><h4 id="letsencrypt"><a href="#letsencrypt" class="headerlink" title="letsencrypt"></a>letsencrypt</h4><h5 id="创建资源clusterissuer"><a href="#创建资源clusterissuer" class="headerlink" title="创建资源clusterissuer"></a>创建资源clusterissuer</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ cat clusterissuer.yaml </span><br><span class="line">apiVersion: cert-manager.io/v1alpha2</span><br><span class="line">kind: ClusterIssuer</span><br><span class="line">metadata:</span><br><span class="line">  name: letsencrypt-prod</span><br><span class="line">spec:</span><br><span class="line">  acme:</span><br><span class="line">    server: https://acme-v02.api.letsencrypt.org/directory</span><br><span class="line">    email: 845789131@qq.com</span><br><span class="line">    privateKeySecretRef:</span><br><span class="line">      name: letsencrypt-prod</span><br><span class="line">    solvers:</span><br><span class="line">    - http01:</span><br><span class="line">        ingress:</span><br><span class="line">          class: nginx</span><br></pre></td></tr></table></figure><h5 id="生成证书-1"><a href="#生成证书-1" class="headerlink" title="生成证书"></a>生成证书</h5><p>生成证书方式和其他方式类似，可手动生成，也可以ingress自动生成，参考上节<a href="#jump">生成证书</a></p><blockquote><p>注意：1.letsencrypt通过acme协议自动申请证书，其中包含两个sloving Challenge: http01,dns01,主要用来证明域名是属于你所有。</p><ol start="2"><li>http01的校验原理是给你域名指向的 HTTP 服务增加一个临时 location ，<code>Let’s Encrypt</code> 会发送 http 请求到 <code>http:///.well-known/acme-challenge/</code>，<code>YOUR_DOMAIN</code> 就是被校验的域名，<code>TOKEN</code>是 ACME 协议的客户端负责放置的文件，在这里 ACME 客户端就是 cert-manager，它通过修改 Ingress 规则来增加这个临时校验路径并指向提供 <code>TOKEN</code> 的服务。此方法仅适用于给使用 Ingress 暴露流量的服务颁发证书，并且不支持泛域名证书。</li><li>dns01 的校验原理是利用 DNS 提供商的 API Key 拿到你的 DNS 控制权限， 在 Let’s Encrypt 为 ACME 客户端提供令牌后，ACME 客户端 (cert-manager) 将创建从该令牌和您的帐户密钥派生的 TXT 记录，并将该记录放在 <code>_acme-challenge.</code>。 然后 Let’s Encrypt 将向 DNS 系统查询该记录，如果找到匹配项，就可以颁发证书。此方法不需要你的服务使用 Ingress，并且支持泛域名证书。</li></ol><p>​            </p></blockquote><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><ul><li>查看ingress-controller访问方式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc -n ingress-nginx</span><br><span class="line">NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">ingress-nginx   NodePort   10.233.21.195   &lt;none&gt;        80:30773/TCP,443:30762/TCP   13d</span><br></pre></td></tr></table></figure><ul><li>添加 /etc/hosts解析（若有内部DNS，可配置DNS）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/hosts</span><br><span class="line">3.1.20.110 nginx.ning.com</span><br></pre></td></tr></table></figure><blockquote><p>注意：由于ingress-controller是nodeport暴露方式，这里需要写k8s其中一个node的IP</p></blockquote><ul><li>访问url</li></ul><p>浏览器打开访问：<a href="https://nginx.ning.com:30762">https://nginx.ning.com:30762</a></p><blockquote><p>注意：此处端口为ingress-controller暴露的443的nodeport端口，而非80</p></blockquote><p>官网介绍： <a href="https://cert-manager.io/docs/configuration/acme/">https://cert-manager.io/docs/configuration/acme/</a></p>]]></content>
    
    <summary type="html">
    
      k8s证书管理软件
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="cert-manager" scheme="http://huaqiang.art/tags/cert-manager/"/>
    
  </entry>
  
  <entry>
    <title>rook使用cephfs</title>
    <link href="http://huaqiang.art/2020/02/17/rook-cephfs/"/>
    <id>http://huaqiang.art/2020/02/17/rook-cephfs/</id>
    <published>2020-02-17T12:36:21.000Z</published>
    <updated>2020-04-04T07:37:24.295Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ul><li>默认安装好了kubernetes集群</li><li>默认已经安装好了rook,安装rook参考上篇<code>rook安装及使用ceph rbd</code><a id="more"></a></br></li><li><input checked="" disabled="" type="checkbox"> 注意: k8s out tree存储有两种方式csi及flexvolume，此文件使用flexvolume，不过csi才是未来。</li></ul><h2 id="创建文件系统"><a href="#创建文件系统" class="headerlink" title="创建文件系统"></a>创建文件系统</h2><p>编排文件如下<code>filesystem.yaml</code>,其中failureDomain为host且replicated是3时，则osd至少在3个主机上，即ceph至少需要三个存储主机。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">CephFilesystem</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">myfs</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># The metadata pool spec. Must use replication.</span></span><br><span class="line"><span class="attr">  metadataPool:</span></span><br><span class="line"><span class="attr">    replicated:</span></span><br><span class="line"><span class="attr">      size:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># The list of data pool specs. Can use replication or erasure coding.</span></span><br><span class="line"><span class="attr">  dataPools:</span></span><br><span class="line"><span class="attr">    - failureDomain:</span> <span class="string">host</span></span><br><span class="line"><span class="attr">      replicated:</span></span><br><span class="line"><span class="attr">        size:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># The metadata service (mds) configuration</span></span><br><span class="line"><span class="attr">  metadataServer:</span></span><br><span class="line">    <span class="comment"># The number of active MDS instances</span></span><br><span class="line"><span class="attr">    activeCount:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.</span></span><br><span class="line">    <span class="comment"># If false, standbys will be available, but will not have a warm cache.</span></span><br><span class="line"><span class="attr">    activeStandby:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># The affinity rules to apply to the mds deployment</span></span><br><span class="line"><span class="attr">    placement:</span></span><br><span class="line">    <span class="comment">#  nodeAffinity:</span></span><br><span class="line">    <span class="comment">#    requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">    <span class="comment">#      nodeSelectorTerms:</span></span><br><span class="line">    <span class="comment">#      - matchExpressions:</span></span><br><span class="line">    <span class="comment">#        - key: role</span></span><br><span class="line">    <span class="comment">#          operator: In</span></span><br><span class="line">    <span class="comment">#          values:</span></span><br><span class="line">    <span class="comment">#          - mds-node</span></span><br><span class="line">    <span class="comment">#  tolerations:</span></span><br><span class="line">    <span class="comment">#  - key: mds-node</span></span><br><span class="line">    <span class="comment">#    operator: Exists</span></span><br><span class="line">    <span class="comment">#  podAffinity:</span></span><br><span class="line">    <span class="comment">#  podAntiAffinity:</span></span><br><span class="line">    <span class="comment"># A key/value list of annotations</span></span><br><span class="line"><span class="attr">    annotations:</span></span><br><span class="line">    <span class="comment">#  key: value</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line">    <span class="comment"># The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory</span></span><br><span class="line">    <span class="comment">#  limits:</span></span><br><span class="line">    <span class="comment">#    cpu: "500m"</span></span><br><span class="line">    <span class="comment">#    memory: "1024Mi"</span></span><br><span class="line">    <span class="comment">#  requests:</span></span><br><span class="line">    <span class="comment">#    cpu: "500m"</span></span><br><span class="line">    <span class="comment">#    memory: "1024Mi"</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f filesystem.yaml</span><br></pre></td></tr></table></figure><h2 id="deployment直接使用"><a href="#deployment直接使用" class="headerlink" title="deployment直接使用"></a>deployment直接使用</h2><p>此deployment中的pod共享文件目录，即/var/lib/registry在三个pod中是都可以读写的。编排文件<code>registry.yaml</code>如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-registry</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line">        <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">registry:2</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line">        <span class="comment"># Configuration reference: https://docs.docker.com/registry/configuration/</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_HTTP_ADDR</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">:5000</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_HTTP_SECRET</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"Ple4seCh4ngeThisN0tAVerySecretV4lue"</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">image-store</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">image-store</span></span><br><span class="line"><span class="attr">        flexVolume:</span></span><br><span class="line"><span class="attr">          driver:</span> <span class="string">ceph.rook.io/rook</span></span><br><span class="line"><span class="attr">          fsType:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">          options:</span></span><br><span class="line"><span class="attr">            fsName:</span> <span class="string">myfs</span> <span class="comment"># name of the filesystem specified in the filesystem CRD.</span></span><br><span class="line"><span class="attr">            clusterNamespace:</span> <span class="string">rook-ceph</span> <span class="comment"># namespace where the Rook cluster is deployed</span></span><br><span class="line">            <span class="comment"># by default the path is /, but you can override and mount a specific path of the filesystem by using the path attribute</span></span><br><span class="line">            <span class="comment"># the path must exist on the filesystem, otherwise mounting the filesystem at that path will fail</span></span><br><span class="line">            <span class="comment">#path: /some/path/inside/cephfs</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f registry.yaml</span><br></pre></td></tr></table></figure><h2 id="通过pv及pvc使用"><a href="#通过pv及pvc使用" class="headerlink" title="通过pv及pvc使用"></a>通过pv及pvc使用</h2><h3 id="创建pv"><a href="#创建pv" class="headerlink" title="创建pv"></a>创建pv</h3><p>pv编排文件如下<code>pv.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cephfs-pv1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">cephfs</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteMany</span></span><br><span class="line"><span class="attr">  flexVolume:</span></span><br><span class="line"><span class="attr">    driver:</span> <span class="string">ceph.rook.io/rook</span></span><br><span class="line"><span class="attr">    fsType:</span> <span class="string">ceph</span></span><br><span class="line"><span class="attr">    options:</span></span><br><span class="line"><span class="attr">      clusterNamespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">      fsName:</span> <span class="string">myfs</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">/some/bigdata/flex</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f pv.yaml</span><br></pre></td></tr></table></figure><h3 id="创建pvc"><a href="#创建pvc" class="headerlink" title="创建pvc"></a>创建pvc</h3><p>pvc编排文件如下<code>pvc.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pvc-nas</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteMany</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">cephfs</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f pvc.yaml</span><br></pre></td></tr></table></figure><h3 id="创建deployment"><a href="#创建deployment" class="headerlink" title="创建deployment"></a>创建deployment</h3><p>deployment编排文件如下<code>registry2.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-registry</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    rollingUpdate:</span></span><br><span class="line"><span class="attr">      maxSurge:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line"><span class="attr">      maxUnavailable:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">RollingUpdate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">kube-registry</span></span><br><span class="line">        <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_HTTP_ADDR</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">:5000</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_HTTP_SECRET</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">Ple4seCh4ngeThisN0tAVerySecretV4lue</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">registry:2</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">            scheme:</span> <span class="string">HTTP</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">          successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">5000</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            port:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">            scheme:</span> <span class="string">HTTP</span></span><br><span class="line"><span class="attr">          periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">          successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line"><span class="attr">        terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">image-store</span></span><br><span class="line"><span class="attr">      dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">      schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line"><span class="attr">      securityContext:</span> <span class="string">&#123;&#125;</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">image-store</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">pvc-nas</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f registry2.yaml</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 注意：虽然pv和pvc写了大小，但是在此文件系统中可用大于此值，无法做限制。</li></ul>]]></content>
    
    <summary type="html">
    
      rook是云原生的存储解决方案
    
    </summary>
    
    
      <category term="k8s rook ceph" scheme="http://huaqiang.art/categories/k8s-rook-ceph/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="rook" scheme="http://huaqiang.art/tags/rook/"/>
    
      <category term="分布式存储" scheme="http://huaqiang.art/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>rook安装及使用ceph rbd</title>
    <link href="http://huaqiang.art/2020/02/10/rook-install/"/>
    <id>http://huaqiang.art/2020/02/10/rook-install/</id>
    <published>2020-02-10T13:00:01.000Z</published>
    <updated>2020-04-04T09:52:17.176Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ul><li>默认安装好了kubernetes集群，版本大于1.10</li><li>k8s节点至少提供5G的容量安装软件<a id="more"></a></li></ul><h2 id="下载rook"><a href="#下载rook" class="headerlink" title="下载rook"></a>下载rook</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下载，以1.0.5版本为例</span></span><br><span class="line">$ wget https://github.com/rook/rook/archive/v1.0.5.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解压</span></span><br><span class="line">$ tar -xzvf v1.0.5.tar.gz</span><br></pre></td></tr></table></figure><h2 id="安装rook"><a href="#安装rook" class="headerlink" title="安装rook"></a>安装rook</h2><ol><li>安装相关组件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> rook-1.0.5/cluster/examples/kubernetes/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装common,rook在k8s中需要的role,serviceaccount等资源</span></span><br><span class="line">$ kubectl create -f common.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装operator，相当于rook的大脑，核心程序</span></span><br><span class="line">$ kubectl create -f operator.yaml</span><br></pre></td></tr></table></figure><ol start="2"><li>检查相关pod是否启动<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n rook-ceph get pod</span><br></pre></td></tr></table></figure></li></ol><h2 id="在rook创建ceph集群"><a href="#在rook创建ceph集群" class="headerlink" title="在rook创建ceph集群"></a>在rook创建ceph集群</h2><ol><li>修改目录下cluster-test.yaml文件，使其如下<code>hq-cluster.yaml</code>。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephCluster</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  cephVersion:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">ceph/ceph:v14.2.1-20190430</span></span><br><span class="line"><span class="attr">    allowUnsupported:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  dataDirHostPath:</span> <span class="string">/var/lib/rook</span></span><br><span class="line"><span class="attr">  mon:</span></span><br><span class="line"><span class="attr">    count:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">    allowMultiplePerNode:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  dashboard:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  network:</span></span><br><span class="line"><span class="attr">    hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  rbdMirroring:</span></span><br><span class="line"><span class="attr">    workers:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">  storage:</span> <span class="comment"># cluster level storage configuration and selection</span></span><br><span class="line"><span class="attr">    useAllNodes:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">    useAllDevices:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">    deviceFilter:</span></span><br><span class="line"><span class="attr">    location:</span></span><br><span class="line"><span class="attr">    config:</span></span><br><span class="line"><span class="attr">    nodes:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">"node4"</span></span><br><span class="line"><span class="attr">      devices:</span> <span class="comment"># specific devices to use for storage can be specified for each node</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdb"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdc"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdd"</span></span><br><span class="line"><span class="attr">      config:</span> <span class="comment"># configuration can be specified at the node level which overrides the cluster level config</span></span><br><span class="line"><span class="attr">        storeType:</span> <span class="string">filestore</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">"node5"</span></span><br><span class="line"><span class="attr">      devices:</span> <span class="comment"># specific devices to use for storage can be specified for each node</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdb"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdc"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdd"</span></span><br><span class="line"><span class="attr">      config:</span> <span class="comment"># configuration can be specified at the node level which overrides the cluster level config</span></span><br><span class="line"><span class="attr">        storeType:</span> <span class="string">filestore</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">"node6"</span></span><br><span class="line"><span class="attr">      devices:</span> <span class="comment"># specific devices to use for storage can be specified for each node</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdb"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdc"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"sdd"</span></span><br><span class="line"><span class="attr">      config:</span> <span class="comment"># configuration can be specified at the node level which overrides the cluster level config</span></span><br><span class="line"><span class="attr">        storeType:</span> <span class="string">filestore</span></span><br></pre></td></tr></table></figure>此配置表示:</li></ol><ul><li>useAllNodes: false   不针对所有的节点，而是手动指定节点</li><li>useAllDevices: false  不选择所有空闲的磁盘，而是手动指定</li><li>nodes: 数组格式，指定node4上面的sdb,sdc,sdd三块盘和node5上sdb,sdc,sdd三块盘及node6上sdb,sdc,sdd</li><li>storeType: filestore ceph类型使用filestore</li></ul><ol start="2"><li><p>创建该<code>hq-cluster.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f hq-cluster.yaml</span><br></pre></td></tr></table></figure></li><li><p>检查相关组件是否启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n rook-ceph</span><br><span class="line">NAME                                    READY   STATUS      RESTARTS   AGE</span><br><span class="line">rook-ceph-agent-2zxz4                   1/1     Running     1          60d</span><br><span class="line">rook-ceph-agent-hrj58                   1/1     Running     1          60d</span><br><span class="line">rook-ceph-agent-m5hns                   1/1     Running     0          60d</span><br><span class="line">rook-ceph-mgr<span class="_">-a</span>-84dcffc8f6-hwlcc        1/1     Running     0          12h</span><br><span class="line">rook-ceph-mon<span class="_">-a</span>-967c6dcbd-rtm74         1/1     Running     0          3d15h</span><br><span class="line">rook-ceph-mon-b-b56bcf68c-xkj49         1/1     Running     1          60d</span><br><span class="line">rook-ceph-mon-c-5b9984bccd-jz7fw        1/1     Running     0          57d</span><br><span class="line">rook-ceph-operator-68cb95fc7c-mvczx     1/1     Running     1          3d15h</span><br><span class="line">rook-ceph-osd-0-6cd844b7db-kc7p7        1/1     Running     1          60d</span><br><span class="line">rook-ceph-osd-1-fc784fb6d-nrvpx         1/1     Running     0          3d15h</span><br><span class="line">rook-ceph-osd-2-79cf8d88f6-crcw7        1/1     Running     1          60d</span><br><span class="line">rook-ceph-osd-3-5787545c8-5chm4         1/1     Running     0          3d15h</span><br><span class="line">rook-ceph-osd-4-76b64b8974-ffmnn        1/1     Running     1          60d</span><br><span class="line">rook-ceph-osd-5-99c99748-gk7pn          1/1     Running     0          3d15h</span><br><span class="line">rook-ceph-osd-6-5dbfdf6cb7-sqr2z        1/1     Running     0          4d14h</span><br><span class="line">rook-ceph-osd-7-568f65bf8d-w475v        1/1     Running     0          4d14h</span><br><span class="line">rook-ceph-osd-8-598b9565d5-qldtn        1/1     Running     0          4d14h</span><br><span class="line">rook-ceph-osd-prepare-node4-vqst4       0/2     Completed   0          12h</span><br><span class="line">rook-ceph-osd-prepare-node5-zplb2       0/2     Completed   0          12h</span><br><span class="line">rook-ceph-osd-prepare-node6-nkldv       0/2     Completed   1          12h</span><br><span class="line">rook-discover-4ddcl                     1/1     Running     2          60d</span><br><span class="line">rook-discover-gqcht                     1/1     Running     0          60d</span><br><span class="line">rook-discover-qvwbn                     1/1     Running     1          60d</span><br></pre></td></tr></table></figure></li></ol><h2 id="在k8s中创建storageclass供k8s使用"><a href="#在k8s中创建storageclass供k8s使用" class="headerlink" title="在k8s中创建storageclass供k8s使用"></a>在k8s中创建storageclass供k8s使用</h2><ol><li>当前文件中包含相关资源，直接创建即可</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f storageclass.yaml</span><br></pre></td></tr></table></figure><ol start="2"><li>检查sc是否创建成功</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get sc -n rook-ceph</span><br></pre></td></tr></table></figure><p>此时新创建的storageclass<code>rook-ceph-block</code>便可在k8s中使用了。</p><h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><ol><li>安装ceph-tools，可以执行ceph相关命令<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f toolbox.yaml </span><br><span class="line"></span><br><span class="line"><span class="variable">$kubectl</span> <span class="built_in">exec</span> -ti rook-ceph-tools-6544484c68-m64vz -n </span><br><span class="line">[....]$ ceph status</span><br><span class="line">  cluster:</span><br><span class="line">    id:     db4f6f7a-5606-4a7d-9eba-9b4901cd7a38</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum a,b,c (age 3d)</span><br><span class="line">    mgr: a(active, since 12h)</span><br><span class="line">    mds: myfs:1 &#123;0=myfs<span class="_">-a</span>=up:active&#125; 1 up:standby-replay</span><br><span class="line">    osd: 9 osds: 9 up (since 3d), 9 <span class="keyword">in</span> (since 3d)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   3 pools, 300 pgs</span><br><span class="line">    objects: 5.55k objects, 17 GiB</span><br><span class="line">    usage:   30 GiB used, 366 GiB / 396 GiB avail</span><br><span class="line">    pgs:     300 active+clean</span><br><span class="line"> </span><br><span class="line">  io:</span><br><span class="line">    client:   1.2 KiB/s rd, 5.7 KiB/s wr, 2 op/s rd, 0 op/s wr</span><br></pre></td></tr></table></figure></li><li>dashboard<br>在<code>cluster-test.yaml</code>有  dashboard选项，设置为true，则自动部署dashboard,查看dashboard登陆的svc为<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc -n rook-ceph |grep dashboard</span><br><span class="line">rook-ceph-mgr-dashboard        LoadBalancer   10.233.9.154    3.1.20.51     8443:32600/TCP      60d</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      rook是云原生的存储解决方案,
    
    </summary>
    
    
      <category term="k8s rook ceph" scheme="http://huaqiang.art/categories/k8s-rook-ceph/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="rook" scheme="http://huaqiang.art/tags/rook/"/>
    
      <category term="分布式存储" scheme="http://huaqiang.art/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>切换k8s容器运行时docker到containerd</title>
    <link href="http://huaqiang.art/2020/01/04/containerd/"/>
    <id>http://huaqiang.art/2020/01/04/containerd/</id>
    <published>2020-01-04T07:27:20.000Z</published>
    <updated>2021-01-07T07:10:31.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-kubelet如何使用容器运行时"><a href="#1-kubelet如何使用容器运行时" class="headerlink" title="1. kubelet如何使用容器运行时"></a>1. kubelet如何使用容器运行时</h2><img src="/assets/img/kubelet.png" alt="kubelet" width="700" ><a id="more"></a><p>在早期 rkt 和 docker 争霸时，kubelet 中需要维护两坨代码分别来适配 docker 和 rkt ，这使得 kubelet 每次发布新功能都需要考虑对运行时组件的适配问题，严重拖慢了新版本发布速度。</p><p>容器运行时可能未来越来越多，如果出现新的运行时，kubelet可能还需要适配新的运行时。于是2016年，k8s提出来容器运行时接口CRI（container Runtime Interface）。CRI是对容器操作的一组抽象，只要容器运行时实现了这个接口，kubelet就能通过这个接口来适配他。不过，docker并没有实现这个接口，似乎也不打算实现这个接口，kubelet只能在内部维护一个称之为<code>docker-shim</code>组件，这个组件充当了docker和CRI的转接器，kubelet在创建容器时通过CRI调用<code>docker-shim</code>，而<code>docker-shim</code>再通过http把请求转给docker。</p><blockquote><p>注意: 现版本docker中，已经使用containerd作为底层容器运行时</p></blockquote><p>所以，若改用containerd替代docker，则kubelet创建容器的调用链如红色所示。可以直接通过一次<code>grpc</code>调用containerd。</p><p>通过上面可知k8s要删除docker是可以理解的一部分了。</p><blockquote><p>其实这是谁胳膊粗的问题，若docker的用户多，且大家都使用，估计也不会剔除它</p></blockquote><h2 id="2-迁移kubelet容器运行时"><a href="#2-迁移kubelet容器运行时" class="headerlink" title="2. 迁移kubelet容器运行时"></a>2. 迁移kubelet容器运行时</h2><h3 id="2-1-关闭节点服务"><a href="#2-1-关闭节点服务" class="headerlink" title="2.1 关闭节点服务"></a>2.1 关闭节点服务</h3><p>关闭相关服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">systemctl stop kubelet</span><br><span class="line">systemctl stop docker</span><br><span class="line">systemctl stop containerd</span><br></pre></td></tr></table></figure><p>由于新版本中的docker在安装时默认使用<code>containerd</code>做为后端容器运行时，故不需要再安装<code>containerd</code></p><h3 id="2-2-修改containerd的plugin"><a href="#2-2-修改containerd的plugin" class="headerlink" title="2.2 修改containerd的plugin"></a>2.2 修改containerd的plugin</h3><p>containerd中默认已经实现了cri，但是以plugin的形式。在默认安装的过程中，这个plugin一般是disable状态，需要将其开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 将其注释掉</span></span><br><span class="line"><span class="comment">#disabled_plugins = ["cri"]</span></span><br><span class="line"></span><br><span class="line">version = 2</span><br><span class="line">[plugins.<span class="string">"io.containerd.grpc.v1.cri"</span>]</span><br><span class="line">  sandbox_image = <span class="string">"registry.cn-qingdao.aliyuncs.com/huaqiangk8s/pause:3.2"</span></span><br><span class="line">  [plugins.<span class="string">"io.containerd.grpc.v1.cri"</span>.containerd]</span><br><span class="line">    default_runtime_name = <span class="string">"runc"</span></span><br><span class="line">    [plugins.<span class="string">"io.containerd.grpc.v1.cri"</span>.containerd.runtimes.runc]</span><br><span class="line">      runtime_type = <span class="string">"io.containerd.runc.v2"</span></span><br><span class="line">      [plugins.<span class="string">"io.containerd.grpc.v1.cri"</span>.containerd.runtimes.runc.options]</span><br><span class="line">        SystemdCgroup = <span class="literal">true</span></span><br></pre></td></tr></table></figure><blockquote><p>一般默认cni和stream配置即可满足kubelet运行使用<br>可以通过<code>crictl info</code>查看，但是某些自定义的k8s安装需要配置<br>cni是被containerd-cri调用</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[plugins.cri.cni]</span></span><br><span class="line">    bin_dir = "/opt/cni/bin"</span><br><span class="line">    conf_dir = "/etc/cni/net.d" </span><br></pre></td></tr></table></figure><p>stream是kubectl exec/log等命令建立流转发通道</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"> [plugins.cri]</span></span><br><span class="line"><span class="attr">stream_server_address</span> = <span class="string">"127.0.0.1"</span></span><br><span class="line"><span class="attr">stream_server_port</span> = <span class="string">"0"</span></span><br><span class="line"><span class="attr">enable_tls_streaming</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start containerd</span><br></pre></td></tr></table></figure><h3 id="2-2-安装配置crictl客户端工具"><a href="#2-2-安装配置crictl客户端工具" class="headerlink" title="2.2 安装配置crictl客户端工具"></a>2.2 安装配置crictl客户端工具</h3><p>下载工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.19.0/crictl-v1.19.0-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf crictl-v1.19.0-linux-amd64.tar.gz </span><br><span class="line">mv crictl /usr/<span class="built_in">local</span>/bin/</span><br></pre></td></tr></table></figure><p>配置<br>修改配置文件 <code>/etc/crictl.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@node6 lib]<span class="comment"># cat /etc/crictl.yaml </span></span><br><span class="line">runtime-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">image-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">timeout: 10</span><br><span class="line">debug: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>测试使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node6 lib]<span class="comment"># crictl pods</span></span><br><span class="line">DEBU[0000] get runtime connection                       </span><br><span class="line">DEBU[0000] connect using endpoint <span class="string">'unix:///var/run/containerd/containerd.sock'</span> with <span class="string">'10s'</span> timeout </span><br><span class="line">POD ID              CREATED             STATE               NAME                                              NAMESPACE           ATTEMPT             RUNTIME</span><br></pre></td></tr></table></figure><h3 id="2-3-修改kubelet启动参数"><a href="#2-3-修改kubelet启动参数" class="headerlink" title="2.3 修改kubelet启动参数"></a>2.3 修改kubelet启动参数</h3><p>查看kubelet启动方式，添加启动参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--container-runtime=remote</span><br><span class="line">--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><p>重启kubelet</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><p>查看相关节点的<code>CONTAINER-RUNTIME</code>是否已经修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get no -o wide</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">node1   Ready    master   61d   v1.18.9   3.1.20.120    &lt;none&gt;        CentOS Linux 7 (Core)   5.9.1-1.el7.elrepo.x86_64    docker:&#x2F;&#x2F;19.3.13</span><br><span class="line">node2   Ready    master   61d   v1.18.9   3.1.20.121    &lt;none&gt;        CentOS Linux 7 (Core)   5.9.1-1.el7.elrepo.x86_64    containerd:&#x2F;&#x2F;1.3.7</span><br><span class="line">node3   Ready    master   61d   v1.18.9   3.1.20.122    &lt;none&gt;        CentOS Linux 7 (Core)   5.9.1-1.el7.elrepo.x86_64    docker:&#x2F;&#x2F;19.3.13</span><br><span class="line">node4   Ready    &lt;none&gt;   61d   v1.18.9   3.1.20.123    &lt;none&gt;        CentOS Linux 7 (Core)   5.9.1-1.el7.elrepo.x86_64    docker:&#x2F;&#x2F;19.3.13</span><br><span class="line">node5   Ready    &lt;none&gt;   61d   v1.18.9   3.1.20.124    &lt;none&gt;        CentOS Linux 7 (Core)   5.9.1-1.el7.elrepo.x86_64    docker:&#x2F;&#x2F;19.3.13</span><br><span class="line">node6   Ready    &lt;none&gt;   61d   v1.18.9   3.1.20.125    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.6.1.el7.x86_64   containerd:&#x2F;&#x2F;1.4.3</span><br></pre></td></tr></table></figure><h3 id="kubelet使用docker和containerd不同的架构图"><a href="#kubelet使用docker和containerd不同的架构图" class="headerlink" title="kubelet使用docker和containerd不同的架构图"></a>kubelet使用docker和containerd不同的架构图</h3><img src="/assets/img/kubelet-docker.png" alt="docker" width="700" ><img src="/assets/img/kubelet-containerd.png" alt="cotainerd" width="700" >]]></content>
    
    <summary type="html">
    
      kubernetes在1.20宣布删除docker-shim,如何切换docker到containerd
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="docker" scheme="http://huaqiang.art/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>我为什么会被kubernetes圈粉</title>
    <link href="http://huaqiang.art/2019/08/16/whyk8s/"/>
    <id>http://huaqiang.art/2019/08/16/whyk8s/</id>
    <published>2019-08-16T13:37:20.000Z</published>
    <updated>2020-04-04T07:18:09.863Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>什么是云计算，对于云计算我们到底怎么理解的。是李彦宏说的新瓶装旧酒，还是马化腾说的还需1000年，或者是马云说的充满希望和信息？</p><a id="more"></a><h2 id="云计算的发展"><a href="#云计算的发展" class="headerlink" title="云计算的发展"></a>云计算的发展</h2><p>云计算已经说了几十年了，现在依然是IT人嘴里常提的一个词。至于什么是云计算，网上有很多概念，这里不做概念解释，而是通过自己的理解将其分为几个阶段（不探讨公有云）</p><ul><li><p>第一阶段 每个公司都有自己的数据中心，自己的计算资源，自己的网络资源，自己的存储资源，自己的IT管理模块。</p></li><li><p>第二阶段 云计算提出阶段，公司整合计算资源，网络资源，存储资源，使资源可以按需配置，灵活扩展。在这一阶段公司将资源池化，通过虚拟化技术、云计算管理界面可以在短时间内就绪一套基础环境，而不需要像第一阶段那样又一个冗长的采购审批流程。代表有vmware,openstack，其他各种私有云技术</p></li><li><p>第三阶段，云计算阶段，除了需要达到第二个阶段的资源整合外，还需包含丰富的服务自制理，弹性伸缩等，计算资源，存储资源，网络资源全部由使用者定义，我们再也不讨论运维，不知道是谁在运维，是什么在运维。</p></li><li><p>第四阶段，GOD cometh in that cloud，在我们还不知道我们需要什么的时候，他便为我们制造了一切。</p></li><li><input checked="" disabled="" type="checkbox"> <p>我们目前整处在第三阶段初期的变革之中，催生了DevOps,CICD,AIOPS,NOOPS,serverless等我们憧憬的东西。</p></li></ul><h2 id="云计算和放牛娃"><a href="#云计算和放牛娃" class="headerlink" title="云计算和放牛娃"></a>云计算和放牛娃</h2><h3 id="第一阶段到第二阶段"><a href="#第一阶段到第二阶段" class="headerlink" title="第一阶段到第二阶段"></a>第一阶段到第二阶段</h3><p>  小时候，每年暑假做的最多的一件事就是放牛。那时候每家都有一头水牛。牛的主要工作是耕地，当然需要时也会拉拉车，推推磨。没有经历的人可能不知道，其实牛并不幸苦，它干的活对我们人来说很牛X，但对它来说太小意思了，而且它的工作别说996，334都达不到。而且更多时候是我们在伺候这位牛爸爸。<br>  于是我们就想，能不能每个村共有一头牛，而不是每家都有一头牛，这样我们就爽啦。但是一头牛耕一个村的地还不把它累死。那我们能造一个累不死的超级牛吗？当然不能，我们不是造物主，得符合自然界客观规律。但是一个村有共有牛是个不错的想法，于是老王家就买了10头牛，创建了牛cloud。不想养牛的人，需要用牛的时候可以来租。从此每家的小孩子就不用放牛了，可以用这个时间来学习高科技了。如果我们把每家都理解成小企业，那么老王家就是公有云厂商，像AWS，Azure，阿里云；如果我们把这个村理解成一个企业，那么老王家就是这个公司的IT部门。</p><h3 id="第二阶段到第三阶段"><a href="#第二阶段到第三阶段" class="headerlink" title="第二阶段到第三阶段"></a>第二阶段到第三阶段</h3><p>  到上面大家可能以为就万事大吉了，那你可能想错了，第一个问题：每年春耕的好日子就那么几天，过了那几天耕地下种子就会影响来年的收成，所以每年到这个时候，大家都来抢牛，为了保障能使用上牛，每家申请都是按天的，即使一天就耕那么一小时，有时候申请的牛病了这一天就耽误了。第二个问题：以前小王放牛，一头牛好放，二头牛也能放，但是10头牛就不是那么好放了。<br>  于是我们又想，老王家能不能雇10几个耕地者，这样谁家来申请就不在申请耕牛了，而是申请耕地服务，这样大家就不在看着牛了，而是考虑自己家的地有没有耕，地耕完立马释放了牛。这种方式就可以细粒度的切分资源，进一步提高牛的使用率。至于小王放牛，以前牵着放，现在可以用工具把大批草料弄回家去喂牛。从此每家都不用在那几天抢牛了，小王也不用担心放牛了。<br>  如果牛是操作系统，那么现在牛耕与李家这个事可以理解成容器，牛耕与李家也不绑定是哪头，这头牛不行，就用下一头牛，而且李家也不用担心这头牛不行，或者耕一半生病了，耕地者自然会调度和切换牛。牛耕地运行时就是内存、CPU资源了，每家只需申请牛耕地运行时。小王喂牛方式的改变就是现在运维方式的改变，以前小王和牛朝夕相处，现在牛栏里关的牛是啥颜色可能都不知道，他只需割草喂牛。</p><h3 id="第三阶段到第四阶段"><a href="#第三阶段到第四阶段" class="headerlink" title="第三阶段到第四阶段"></a>第三阶段到第四阶段</h3><p>  到了上面大家可能又认为万事大吉了，那你又错了，牛会生病，耕地者会生病，大家都在学习高科技，小王的孩子小小王不想学他爸爸再和牛有关系了。不过好在学习高科技那些孩子长大了，创造了不吃草，会自己耕地的牛。</p><h3 id="第四阶段"><a href="#第四阶段" class="headerlink" title="第四阶段"></a>第四阶段</h3><p>  小小小小王抬头看了一眼表，刚好12:00。回头看了下自己工位，一份8个腊八蒜，24片肥肠的腊八蒜炒肥肠放在了桌子上。不知道哪儿来的，反正就在那里。</p><ul><li><input checked="" disabled="" type="checkbox"> 在第二阶段到第三阶段中，牛，小王，耕地者，申请耕地的人，他们之间由谁来协调，谁来管理。目前我想比较出色的就是kubernetes了。</li></ul>]]></content>
    
    <summary type="html">
    
      如何理解云计算
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="docker" scheme="http://huaqiang.art/tags/docker/"/>
    
      <category term="云计算" scheme="http://huaqiang.art/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Service mesh 之 初探（一）</title>
    <link href="http://huaqiang.art/2019/07/07/ServiceMesh/"/>
    <id>http://huaqiang.art/2019/07/07/ServiceMesh/</id>
    <published>2019-07-07T12:37:20.000Z</published>
    <updated>2020-04-04T07:35:12.871Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Service mesh在云原生中是不可缺少的一环，幸运的是kubernetes环境下的service mesh解决方案不像我们想象的那么麻烦。</p><a id="more"></a><h3 id="提出"><a href="#提出" class="headerlink" title="提出"></a>提出</h3><p>微服务架构最早只在以架构师为主的少数派群体中谈论，随着容器技术的爆发式发展，微服务架构再次引起了更多人的注意。在微服务架构中，各个微服务之间的流量控制、路由分发、请求监控、高质量的网络传输一直都是采用侵入式的设计思想，但是这种侵入式的开发框架会导致开发工作量的大幅增加。非侵入式的Service Mesh技术的出现，犹如一股春风迎面吹来。</p><p>Service Mesh最早是Buoyant公司提出，该公司由两个Twitter跳槽的工程创办，并且开发了第一个Service Mesh项目linkerd，<br>2016年9月29日Buoyant CEO Willian Morgan演讲时第一次公开使用Service Mesh：</p><blockquote><p>原文：A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.</p></blockquote><blockquote><p>翻译：服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。</p></blockquote><p>2017年初，随着linkerd的传入，Service Mesh进入国内技术社区。<br>国内最早翻译为“服务啮合层”，但是比较拗口，后来逐渐改为“服务网格”</p><h3 id="理解service-Mesh"><a href="#理解service-Mesh" class="headerlink" title="理解service Mesh"></a>理解service Mesh</h3><p>考虑这样一个问题，如果在服务之间处理服务发现、负载均衡、超时、重试等机制该如何处理？</p><ol><li>我们首先想到的应该是每个微服务自己处理，由开发人员开发相关代码，如下图：</li></ol><img src="/assets/img/servicemesh1-1.jpg" width="75%"><ol start="2"><li>上面架构解决了我们的需求，但是每次开发除了业务逻辑外，还处理网络控制，这极大的增加了工作量，甚至有时网络控制这段逻辑比业务代码还要难。于是就有了一些框架，所有服务使用统一的类库进行通讯，比如Netflix oss套件，Spring Cloud框架。这样，开发人员只需少量的代码，甚至几个注释就能搞定，如下图：</li></ol><img src="/assets/img/servicemesh1-2.jpg" width="75%"><ol start="3"><li>在一段时间上面第二种结构满足了微服务架构的需求，但是其对框架的依赖较大，而且不能跨语言。真真的微服务架构，应该是service A无需关注service B是何种开发语言的。随着service mesh的提出，出现了以下架构：</li></ol><img src="/assets/img/servicemesh1-3.jpg" width="75%"><p>在上面架构图中，把微服务之间的交互抽象出一层，做为基础设施层，这样代码的开发完全不需关注网络控制，如服务发现、负责均衡、超时重试等，也不需要每个微服务使用同一种开发语言或者使用同一种开发框架。</p><p>Service Mesh的定义：Service Mesh是一个用于处理服务与服务之间通信的基础架构设施层。</p><ul><li><input checked="" disabled="" type="checkbox"> 其实这种抽象分层的处理思路早在OSI七层模型中就由体现。在我们解决复杂的逻辑是不妨多采用这种思路处理问题。</li></ul>]]></content>
    
    <summary type="html">
    
      服务治理在kubernetes中是非常重要的一块
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="docker" scheme="http://huaqiang.art/tags/docker/"/>
    
      <category term="service mesh" scheme="http://huaqiang.art/tags/service-mesh/"/>
    
  </entry>
  
  <entry>
    <title>prometheus 配置钉钉告警</title>
    <link href="http://huaqiang.art/2019/05/28/alertmanager/"/>
    <id>http://huaqiang.art/2019/05/28/alertmanager/</id>
    <published>2019-05-28T12:30:20.000Z</published>
    <updated>2020-04-04T10:01:08.095Z</updated>
    
    <content type="html"><![CDATA[<p>prometheus的alertmanager本身不支持直接使用钉钉告警的方式，若配置钉钉告警需要webhook插件做为组件。</p><a id="more"></a><h3 id="安装钉钉告警webhook插件"><a href="#安装钉钉告警webhook插件" class="headerlink" title="安装钉钉告警webhook插件"></a>安装钉钉告警webhook插件</h3><p>由于我的prometheus和alertmanager都安装在kubernetes里面，所以钉钉插件也安装在k8s平台里面。若需要其他安装方式，可以参考<br><a href="https://github.com/timonwong/prometheus-webhook-dingtalk">钉钉插件github仓库</a></p><ul><li>vim dingtalk-alert-deployment.yml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: dingtalk-alert</span><br><span class="line">  name: dingtalk-alert</span><br><span class="line">  namespace: monitor</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: dingtalk-alert</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: dingtalk-alert</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: timonwong&#x2F;prometheus-webhook-dingtalk</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;bin&#x2F;prometheus-webhook-dingtalk</span><br><span class="line">        - --ding.profile&#x3D;webhook1&#x3D;https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;da1cc37cd155f73112bcbf4aa4be49c8c400786f1b38908a15fa1e9be0eee51</span><br><span class="line">        - --template.file&#x3D;&#x2F;usr&#x2F;share&#x2F;prometheus-webhook-dingtalk&#x2F;template&#x2F;default.tmpl</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: dingtalk-alert</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8086</span><br><span class="line">          name: service</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 50m</span><br><span class="line">            memory: 128Mi</span><br><span class="line">        terminationMessagePath: &#x2F;dev&#x2F;termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &#x2F;usr&#x2F;share&#x2F;prometheus-webhook-dingtalk&#x2F;template&#x2F;default.tmpl</span><br><span class="line">          name: dingtalk-tmpl</span><br><span class="line">          subPath: default.tmpl</span><br><span class="line">      dnsPolicy: ClusterFirst</span><br><span class="line">      volumes:</span><br><span class="line">      - configMap:</span><br><span class="line">          name: dingtalk-alert-configmap</span><br><span class="line">        name: dingtalk-tmpl</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 注意： –ding.profile指定钉钉机器人的url，其中webhook1为自定义名称，在配置alertmanager规则中指定webhook时会使用</li></ul><ul><li><input checked="" disabled="" type="checkbox"> 注意：–template.file指定发送告警模版文件</li></ul><ul><li>[x]注意：–ding.profile后钉钉机器人url不可有引号，不然会报404错误，非k8s安装时需要加引号</li></ul><ul><li>vim dingtalk-configmap.yml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  default.tmpl: |</span><br><span class="line">    &#123;&#123; define &quot;__subject&quot; &#125;&#125;[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;] &#123;&#123; .GroupLabels.SortedPairs.Values | join &quot; &quot; &#125;&#125; &#123;&#123; if gt (len .CommonLabels) (len .GroupLabels) &#125;&#125;(&#123;&#123; with .CommonLabels.Remove .GroupLabels.Names &#125;&#125;&#123;&#123; .Values | join &quot; &quot; &#125;&#125;&#123;&#123; end &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;</span><br><span class="line">    &#123;&#123; define &quot;__alertmanagerURL&quot; &#125;&#125;&#123;&#123; .ExternalURL &#125;&#125;&#x2F;#&#x2F;alerts?receiver&#x3D;&#123;&#123; .Receiver &#125;&#125;&#123;&#123; end &#125;&#125;</span><br><span class="line"></span><br><span class="line">    &#123;&#123; define &quot;__text_alert_list&quot; &#125;&#125;&#123;&#123; range . &#125;&#125;</span><br><span class="line">    **告警**</span><br><span class="line">    &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;</span><br><span class="line">    &#123;&#123; end &#125;&#125;</span><br><span class="line"></span><br><span class="line">    **标签**</span><br><span class="line">    &#123;&#123; range .Labels.SortedPairs &#125;&#125;&gt; - &#123;&#123; .Name &#125;&#125;: &#123;&#123; .Value | markdown | html &#125;&#125;</span><br><span class="line">    &#123;&#123; end &#125;&#125;</span><br><span class="line">    **Source:** **http:&#x2F;&#x2F;grafana.monitor.bj.univer.top&#x2F;d&#x2F;xLrfYirmk&#x2F;a-li-yun-gao-jing?orgId&#x3D;1**</span><br><span class="line"></span><br><span class="line">    &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;</span><br><span class="line"></span><br><span class="line">    &#123;&#123; define &quot;ding.link.title&quot; &#125;&#125;&#123;&#123; template &quot;__subject&quot; . &#125;&#125;&#123;&#123; end &#125;&#125;</span><br><span class="line">    &#123;&#123; define &quot;ding.link.content&quot; &#125;&#125;#### \[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status &quot;firing&quot; &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;\] **[&#123;&#123; index .GroupLabels &quot;alertname&quot; &#125;&#125;](&#123;&#123; template &quot;__alertmanagerURL&quot; . &#125;&#125;)**</span><br><span class="line">    &#123;&#123; template &quot;__text_alert_list&quot; .Alerts.Firing &#125;&#125;</span><br><span class="line">    &#123;&#123; end &#125;&#125;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-05-13T07:15:10Z</span><br><span class="line">  labels:</span><br><span class="line">    app: dingtalk-alert</span><br><span class="line">  name: dingtalk-alert-configmap</span><br><span class="line">  namespace: monitor</span><br></pre></td></tr></table></figure></li><li>vim dingtalk-service.yml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-05-13T07:15:10Z</span><br><span class="line">  labels:</span><br><span class="line">    app: dingtalk-alert</span><br><span class="line">  name: dingtalk-alert-svc</span><br><span class="line">  namespace: monitor</span><br><span class="line">  resourceVersion: &quot;31476279&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;monitor&#x2F;services&#x2F;dingtalk-alert-svc</span><br><span class="line">  uid: d81b6783-754e-11e9-8566-00163e10fec4</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 172.19.1.47</span><br><span class="line">  externalTrafficPolicy: Cluster</span><br><span class="line">  ports:</span><br><span class="line">  - name: service</span><br><span class="line">    nodePort: 31221</span><br><span class="line">    port: 8060</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8060</span><br><span class="line">  selector:</span><br><span class="line">    app: dingtalk-alert</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure></li></ul><h3 id="修改alertmanager配置"><a href="#修改alertmanager配置" class="headerlink" title="修改alertmanager配置"></a>修改alertmanager配置</h3><ul><li>alertmanager增加webhook选项，指定钉钉插件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">alertmanager.yml: |</span><br><span class="line">  global:</span><br><span class="line">      resolve_timeout: 5m</span><br><span class="line">  receivers:</span><br><span class="line">  - name: webhook</span><br><span class="line">    webhook_configs:</span><br><span class="line">    - url: http:&#x2F;&#x2F;dingtalk-alert-svc:8060&#x2F;dingtalk&#x2F;ops&#x2F;send</span><br><span class="line">      send_resolved: true</span><br><span class="line">  route:</span><br><span class="line">    group_interval: 5m</span><br><span class="line">    group_wait: 10s</span><br><span class="line">    receiver: webhook</span><br><span class="line">    repeat_interval: 3h</span><br></pre></td></tr></table></figure><h3 id="整体结构如下"><a href="#整体结构如下" class="headerlink" title="整体结构如下"></a>整体结构如下</h3><img src="/assets/img/altermanager.jpg" width="75%"></li></ul>]]></content>
    
    <summary type="html">
    
      alertmanager是prometheus中的告警模块，可以支持微信、钉钉等主流应用。
    
    </summary>
    
    
      <category term="k8s" scheme="http://huaqiang.art/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://huaqiang.art/tags/k8s/"/>
    
      <category term="prometheus" scheme="http://huaqiang.art/tags/prometheus/"/>
    
      <category term="监控" scheme="http://huaqiang.art/tags/%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>docker中的top命令</title>
    <link href="http://huaqiang.art/2019/05/23/top/"/>
    <id>http://huaqiang.art/2019/05/23/top/</id>
    <published>2019-05-23T12:37:20.000Z</published>
    <updated>2020-04-04T07:16:14.242Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>客户交流中被问到的两个问题<br>1.在docker中运行top命令能否看到宿主机的内存CPU信息？<br>2.op能看到容器外面宿主机的进程吗？</p><a id="more"></a><h4 id="提问：在docker中运行top命令能否看到宿主机的内存CPU信息？"><a href="#提问：在docker中运行top命令能否看到宿主机的内存CPU信息？" class="headerlink" title="提问：在docker中运行top命令能否看到宿主机的内存CPU信息？"></a>提问：在docker中运行top命令能否看到宿主机的内存CPU信息？</h4><p>回答这个问题之前我们首先想以下linux系统中的/proc目录是干什么的</p><p>/proc是个伪文件系统，存在于内存之中而不是硬盘上。通过它可以和内核内部数据结构进行交互，获取 有关进程的有用信息。<br>/proc中会有以下文件:</p><ul><li>/proc/cpuinfo - CPU 的信息(型号, 家族, 缓存大小等)</li><li>/proc/meminfo - 物理内存、交换空间等的信息</li><li>/proc/mounts - 已加载的文件系统的列表</li><li>/proc/devices - 可用设备的列表</li><li>/proc/filesystems - 被支持的文件系统</li><li>/proc/modules - 已加载的模块</li><li>/proc/version - 内核版本</li><li>/proc/cmdline - 系统启动时输入的内核命令行参数</li><li>/proc/{pid} - 进程信息</li></ul><p>top正是通过/proc来获取当前内存、CPU信息的。dock er启动的容器共享操作系统内核，在容器启动的时候会挂载部分/proc里面的文件，能否通过top在docker中看到内存和CPU信息，主要看有没有挂载这些文件。</p><p>那么问题又来了，需要手动挂载这些文件吗？一般来说不需要，挂载内容的多少一般和镜像有关。一般的redhat/ubuntu镜像都是会挂载内存CPU信息的，精简的dokcer镜像信息会少些，有时top得到的信息不全面。</p><h4 id="另外一个问题，top能看到容器外面宿主机的进程吗？"><a href="#另外一个问题，top能看到容器外面宿主机的进程吗？" class="headerlink" title="另外一个问题，top能看到容器外面宿主机的进程吗？"></a>另外一个问题，top能看到容器外面宿主机的进程吗？</h4><p>经过上面分析top看到信息的多少取决于/proc里面内容，由于pid 命名空间的隔离，宿主机的pid信息不会被启动容器挂载，所以看不到容器外宿主机进程。</p><ul><li><input disabled="" type="checkbox"> free -m命令原理和top类似</li></ul>]]></content>
    
    <summary type="html">
    
      docker中执行top命令能否看到宿主机内存CPU信息
    
    </summary>
    
    
      <category term="docker" scheme="http://huaqiang.art/categories/docker/"/>
    
    
      <category term="linux" scheme="http://huaqiang.art/tags/linux/"/>
    
      <category term="docker" scheme="http://huaqiang.art/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>理解系统中的cache &amp; buffer</title>
    <link href="http://huaqiang.art/2019/05/21/buffer_cache/"/>
    <id>http://huaqiang.art/2019/05/21/buffer_cache/</id>
    <published>2019-05-21T12:37:20.000Z</published>
    <updated>2020-04-04T10:00:36.814Z</updated>
    
    <content type="html"><![CDATA[<p>在linux系统优化中，我们经常看到cache/buffer，但是什么是cache,什么是buffer呢，他们之间有没有区别？</p><a id="more"></a><p>从字面意思来看，cache是缓存，buffer是缓冲。这里假设有两个设备，分别叫A和B，由于制作工艺的差别，A的处理速度远远大于B。假设A和B在接口速度上有一定的差别，A接口速度为10M/s,B接口速度为5M/s.现在B中有件事需要A处理，命名为B<sub>1</sub>， 大小为10M，如下图所示:</p><p>A&lt;————-&gt;B(B<sub>1</sub>)</p><p>那么在处理这个事件会出现这种情况，1.A拿到B<sub>1</sub>需要2s，因为B的接口只为5M/s；2.同样A处理完还给B也需要2s。在这个过程中A一半的时间都在等待传输上。</p><p>若何优化这个过程呢，我们可以在A和B之间加一层：</p><p>A&lt;—–&gt;[a——–b]——&gt;B(B<sub>1</sub>,B<sub>2</sub>)</p><p>a的接口速度接近A，b的接口速度至少为B。那么</p><ul><li><p>在处理完B<sub>1</sub>后再处理B<sub>2</sub>，A就可以直接从a拿数据了，若后面还有B<sub>3</sub>，B<sub>4</sub>，B<sub>n</sub>需要处理，这将会大大节省时间，这个主要是读的操作就是是缓存，即cache的理解；</p></li><li><p>以上过程中我们不能忽略了A处理完B<sub>1</sub>往B写的过程。A处理完可以直接写入[a–b]，速度提升了，但不会立刻写到B，在处理完B<sub>2</sub>,B<sub>3</sub> …B<sub>n</sub>之后一起写入B，避免了对B频繁操作。这个主要是写的操作就是缓冲，即buffer的理解；</p></li></ul><p>一张图描述如下：</p><img src="/assets/img/buffer.jpg" width="75%" height="50%"><p>网上有两个有意思的例子：</p><ul><li><p>假设某地发生了自然灾害（比如地震），居民缺衣少食，于是派救火车去给若干个居民点送水。<br>救火车到达第一个居民点，开闸放水，老百姓就拿着盆盆罐罐来接水。<br>假如说救火车在一个居民点停留100分钟放完了水，然后重新储水花半个小时，再开往下一个居民点。这样一个白天来来来回回的，也就是4-5个居民点。<br>但我们想想，救火车是何等存在，如果把水龙头完全打开，其强大的水压能轻易冲上10层楼以上， 10分钟就可以把水全部放完。但因为居民是拿盆罐接水，100%打开水龙头那就是给人洗澡了，所以只能打开一小部分（比如10%的流量）。但这样就降低了放水的效率（只有原来的10%了），10分钟变100分钟。<br>那么，我们是否能改进这个放水的过程，让救火车以最高效率放完水、尽快赶往下一个居民点呢？<br>方法就是：在居民点建蓄水池。<br>救火车把水放到蓄水池里，因为是以100%的效率放水，10分钟结束然后走人。居民再从蓄水池里一点一点的接水。<br>我们分析一下这个例子，就可以知道Cache的含义了。<br>救火车要给居民送水，居民要从救火车接水，就是说居民和救火车之间有交互，有联系。<br>但救火车是“高速设备”，居民是“低速设备”，低速的居民跟不上高速的救火车，所以救火车被迫降低了放水速度以适应居民。<br>为了避免这种情况，在救火车和居民之间多了一层“蓄水池（也就是Cache）”，它一方面以100%的高效和救火车打交道，另一方面以10%的低效和居民打交道，这就解放了救火车，让其以最高的效率运行，而不被低速的居民拖后腿，于是救火车只需要在一个居民点停留10分钟就可以了。</p><blockquote><p>从以上例子可以看出，所谓Cache，就是“为了弥补高速设备和低速设备之间的矛盾”而设立的一个中间层。因为在现实里经常出现高速设备要和低速设备打交道，结果被低速设备拖后腿的情况。</p></blockquote></li><li><p>比如说吐鲁番的葡萄熟了，要用大卡车装葡萄运出去卖<br>果园的姑娘采摘葡萄，当然不是前手把葡萄摘下来,后手就放到卡车上，而是需要一个中间过程“箩筐”：摘葡萄→放到箩筐里→把箩筐里的葡萄倒入卡车。<br>也就是说，虽然最终目的是“把葡萄倒入卡车”，但中间必须要经过“箩筐”的转手，这里的箩筐就是Buffer。</p><blockquote><p>从以上例子可以看出，所谓buffer,就是是“暂时存放物品的空间”。它的引入是为了减小短期内突发I/O的影响，起到流量整形的作用。</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      cache/buffer,系统优化必不可少的部分
    
    </summary>
    
    
      <category term="操作系统" scheme="http://huaqiang.art/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="linux" scheme="http://huaqiang.art/tags/linux/"/>
    
      <category term="概念" scheme="http://huaqiang.art/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>灾备的终极目标</title>
    <link href="http://huaqiang.art/2019/05/17/it/"/>
    <id>http://huaqiang.art/2019/05/17/it/</id>
    <published>2019-05-17T14:37:20.000Z</published>
    <updated>2020-04-04T07:37:51.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是灾备"><a href="#什么是灾备" class="headerlink" title="什么是灾备"></a>什么是灾备</h2><p>灾备主要分为三个发展阶段：</p><ul><li>整个灾备行业的起源应该是在70年代，当时主要关注的是IT系统数据、主要做的便是数据备份；<a id="more"></a></li><li>随着科技发展，IT备份发展到了灾难恢复规划，也就是DRP阶段，这一阶段不仅仅是数据备份了，还包括应急预案、灾备中心管理等；</li><li>再后来，人们通过业务连续性来衡量灾备的目标，也就是从DRP阶段到BCP阶段，在预案管理、灾备数据中心管理之上再加上面向业务的决策、公关等。<blockquote><p>总结下：第一阶段就是数据备份，工程师就可以干的；第二阶段需要建设灾备数据中心，灾备架构设计等，这个就需要至少IT负责人才能干；第三阶段涉及到业务，包括灾难发生后的各种应急预案，危机公关等，这个至少需要CXO以上级别参与才能办的到。</p></blockquote></li></ul><h2 id="RTO-RPO"><a href="#RTO-RPO" class="headerlink" title="RTO/RPO"></a>RTO/RPO</h2><ul><li><p>RTO：Recovery Time Objective,是指灾难发生后，从IT系统宕机导致业务停顿之时开始，到IT系统恢复至可以支持各部门运作、恢复运营之时，此两点之间的时间段。</p></li><li><p>RPO：Recovery Point Objective,是指灾难发生后，容灾系统能把数据恢复到灾难发生前时间点的数据。是衡量灾难发生后会丢失多少生产数据的指标。可简单的描述为设施能容忍的最大数据丢失量。</p></li></ul><blockquote><p>RPO衡量的是IT系统丢失的数据量（通过时间衡量）；RTO衡量的是IT系统不可用时间，现在也有业务RTO说法，指的是业务系统不可用时间（发生灾难业务系统不可用到下一次可用时间）</p></blockquote><ul><li><input checked="" disabled="" type="checkbox"> 使RTO和RPO无限趋向于0，是灾备的终极目标！</li></ul><h2 id="怎么做灾备"><a href="#怎么做灾备" class="headerlink" title="怎么做灾备"></a>怎么做灾备</h2><p>灾备怎么做？个人认为还是从第一节的三个角色给出建议。如果您是一个系统工程师或者数据库管理员，您的第一要务便是备份，永远记住不断向领导提备份重要性，做任何变更前第一件事就是备份。如果您是个IT负责人，您的第一要务就是数据安全，工程师的备份可能无法满足您对数据中心数据安全的需求，那么您一定得考虑容灾，容灾做成什么程度取决于您的预算和RPO/RTO要求。如果您是公司CEO，而您公司的业务系统承载了你们公司核心业务，那么您必须得过问你们数据中心业务系统的容灾，因为这可能影响你们公司的生死存亡。</p><p>这里对于灾备分为备份和容灾两部分：</p><ul><li>备份：备份主要考虑两点，一个是备份工具选择；另外一个就是备份计划制定。备份计划除了临时备份，一定得考虑周期性，备份工具可选择较多，以下举例几种</li></ul><table><thead><tr><th>备份工具</th><th>使用场景</th></tr></thead><tbody><tr><td>acronis</td><td>一款俄罗斯的备份工具，功能强大，基本能备份任何系统，甚至android系统</td></tr><tr><td>veeam</td><td>用于备份vmware和hyper-v,无需vm中安装Agent,备份恢复速度较快</td></tr><tr><td>NBU</td><td>赛门铁克企业级备份工具，稳定耐用，适合中大型企业，不过比较贵</td></tr><tr><td>BE</td><td>赛门铁克企业级备份工具，比NBU弱一些，适合3-100台服务器的中型企业</td></tr><tr><td>Amanda</td><td>最早出现的开源备份软件,通过系统命令执行备份，免费使用，也可以选择其服务版本zmanda</td></tr><tr><td>Bacula</td><td>开源的跨平台网络备份工具，它提供了基于企业级的客户端/服务器的备份恢复解决方案,也有商业版本，可提供操作界面</td></tr><tr><td>Asigra</td><td>无代理备份工具，物理机虚机均不需要代理（在同一个子网不需要输入用户名密码就可以扫描到其他主机目录，相当牛逼），使用需要硬件key</td></tr><tr><td>CommVault</td><td>没用过，也是个很给力的备份工具</td></tr></tbody></table><ul><li>容灾：容灾主要考虑三点，第一是容灾数据中心选择；第二是数据复制工具选择；第三是DRP，即灾难恢复计划。容灾主要讲究的是解决方案，但是第二个数据复制技术和工具选择是灵魂，以下列举几个感觉不错的容灾工具</li></ul><table><thead><tr><th>容灾工具</th><th>使用场景</th></tr></thead><tbody><tr><td>SRM</td><td>vmware自带解决方案，价格合适，RPO分钟级，RTO分钟级，无需安装Agent</td></tr><tr><td>Zerto</td><td>vmware、hyper-v解决方案，可以支持到AWS，Azure公有云灾备，CDP技术，RPO秒级，RTO分钟级别，无需安装Agent</td></tr><tr><td>飞康</td><td>CDP技术，存储级别解决方案，对带宽要求较大，RPO秒级、RTO秒级</td></tr><tr><td>Double-Take</td><td>OS级别解决方案，需要安装Agent，RPO秒级，RTO分钟级（测试有时是小时级别）</td></tr><tr><td>ARCserver</td><td>mysql,oracle,sqlserver解决方案，需要安装Agent，RPO秒级，RTO秒级</td></tr><tr><td>英方</td><td>和double-take类似</td></tr><tr><td>OceanMirror</td><td>国产软件，类似ARCserver，比ARCserver便宜</td></tr><tr><td>DTS</td><td>阿里云数据库灾备解决方案</td></tr><tr><td>Dataguard</td><td>oracle数据库容灾解决方案</td></tr><tr><td>AB复制</td><td>mysql容灾解决方案</td></tr></tbody></table><blockquote><p>在实际项目中，容灾备份很多时候在规划中，直到出现一次问题后才提上日程。做为一个专业的IT负责人应该把风险在出问题之前解决掉，而不是事后补救。</p></blockquote><h2 id="未来灾备方向"><a href="#未来灾备方向" class="headerlink" title="未来灾备方向"></a>未来灾备方向</h2><p>我们之前考虑容灾的数据复制自下而上分为，存储级别（飞康、vplex），hypervisor级别(zerto、SRM)，OS级别（DT、英方），软件级别（OM、ARCserver、DG)。主要解决的冲突为，硬件故障，操作系统故障，软件故障，业务数据丢失（分为结构化和非结构化数据）等。现在比较火的是容器技术和公有云，容积技术确实适合搭建未来IT所需的超级计算集群。</p><ol><li>由于计算集群搭建，云的概念越来越普及，硬件级别故障，os级别故障已经在云计算架构设计中解决了，不需对单个业务系统再做容灾保护。</li><li>在计算集群中非结构化数据倾向存于分布式存储，而分布式存储在设计时便考虑了容灾的特性。所以未来对非结构化数据容灾将越来越少。</li><li>结构化数据存储，对非关系型数据库需求增大，关系型数据库需求逐渐减小。</li></ol><blockquote><p>所以，个人认为未来灾备重心应该主要是在数据中心级别的分布式存储或分布式数据库架构设计上，部分在关系型数据库上。</p></blockquote>]]></content>
    
    <summary type="html">
    
      灾备是一个容易忽略的重要板块
    
    </summary>
    
    
      <category term="IT理念" scheme="http://huaqiang.art/categories/IT%E7%90%86%E5%BF%B5/"/>
    
    
      <category term="概念" scheme="http://huaqiang.art/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="灾备" scheme="http://huaqiang.art/tags/%E7%81%BE%E5%A4%87/"/>
    
  </entry>
  
  <entry>
    <title>静态博客之hexo</title>
    <link href="http://huaqiang.art/2018/05/18/hexo/"/>
    <id>http://huaqiang.art/2018/05/18/hexo/</id>
    <published>2018-05-18T14:37:20.000Z</published>
    <updated>2020-04-04T09:52:02.757Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个测试博客,若想知道更多hexo信息，请点击以下Quick Start中相关链接。</p><a id="more"></a><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一个测试博客,若想知道更多hexo信息，请点击以下Quick Start中相关链接。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
